{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OIjOPWowcJq",
    "outputId": "5973e15b-5c3f-488b-8dd0-f6ff46330e8c"
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q transformers datasets  # 2 Hugging Face libraries\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ync6uXy-wcJs"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from contextlib             import suppress\n",
    "from datasets               import DatasetDict\n",
    "from google.colab           import auth, drive, files, userdata\n",
    "from huggingface_hub        import create_branch, create_repo, HfApi, login, upload_file, hf_hub_download, whoami\n",
    "from huggingface_hub.errors import RepositoryNotFoundError\n",
    "from sklearn.metrics        import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score, accuracy_score, hamming_loss, classification_report,  precision_recall_fscore_support\n",
    "from torch.optim            import AdamW\n",
    "from torch.utils.data       import DataLoader\n",
    "from tqdm.auto              import tqdm\n",
    "from transformers           import EvalPrediction, LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainerCallback, TrainingArguments\n",
    "#from transformers           import logging as transformers_logging\n",
    "from torch.nn               import BCEWithLogitsLoss, Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8E2BDdPU394"
   },
   "outputs": [],
   "source": [
    "skills         = 791112\n",
    "all_rows_low   = 0\n",
    "all_rows_high  = 50000 # 120 1200 12000 24000 36000 48000\n",
    "\n",
    "threshold_tuning = True\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# 1) GIVE ACCESS TO HF AND WANDB\n",
    "# 2) ADAPT THE HF HUB REPO_ID\n",
    "# 1) ADAPT THE MODEL CARD\n",
    "# duration for 7 epochs: 3h 40m\n",
    "# duration for 5 epochs: 2h 40m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQJHNH0zeS41",
    "outputId": "8bb49ce7-d0e2-4955-9e39-7992bab9354e"
   },
   "outputs": [],
   "source": [
    "# Is /content the current directory?\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Ensure the logs directory exists\n",
    "os.makedirs('/content/logs', exist_ok=True)\n",
    "\n",
    "# Create handlers for both file and console output\n",
    "file_handler = logging.FileHandler('/content/logs/training.log')\n",
    "file_handler.setLevel(logging.INFO)                               # Log level for file\n",
    "\n",
    "#console_handler = logging.StreamHandler()\n",
    "#console_handler.setLevel(logging.INFO)                            # Log level for console\n",
    "\n",
    "# Create a formatter and attach it to both handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "#console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handlers to the root logger\n",
    "logger = logging.getLogger()\n",
    "#logger.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "#logger.addHandler(console_handler)\n",
    "\n",
    "#transformers_logging.set_verbosity_error()  # Only show errors, suppress info\n",
    "\n",
    "# Remove all existing handlers of type StreamHandler\n",
    "#for handler in logger.handlers[:]:\n",
    "#    if isinstance(handler, logging.StreamHandler):\n",
    "#        logger.removeHandler(handler)\n",
    "\n",
    "# Verify by printing current handlers\n",
    "#print(\"handlers\", logger.handlers)  # Should only show FileHandler\n",
    "\n",
    "# Test logging to ensure both file and console work\n",
    "logging.info(\"Testing log before training starts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXMnrvto55rW"
   },
   "source": [
    "## Google Cloud Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLG2EnRQ53G4"
   },
   "outputs": [],
   "source": [
    "#auth.authenticate_user()  # user = c.lepere@ictjob.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nkZXVHOruti"
   },
   "source": [
    "## Get skills and jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCDIh_QXrmdI",
    "outputId": "1e431f93-1a9e-4c4f-8a19-8fd42c6af24b"
   },
   "outputs": [],
   "source": [
    "#skills         = 791112\n",
    "#all_rows_low   = 0\n",
    "#all_rows_high  = 10000 # 120 1200 12000 24000 36000 48000\n",
    "num_datapoints = all_rows_high - all_rows_low\n",
    "\n",
    "datasetDict_zip_file_name = f\"dataset_EN_{skills}_{all_rows_low}_{all_rows_high}.zip\"\n",
    "datasetDict_dir_name      = os.path.splitext(datasetDict_zip_file_name)[0]\n",
    "\n",
    "print(f\"datasetDict_zip_file_name: {datasetDict_zip_file_name}\")\n",
    "print(f\"datasetDict_dir_name     : {datasetDict_dir_name}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjjlUtVRMc_R"
   },
   "source": [
    "## Averages\n",
    "<pre>\n",
    "- per sample: metrics are computed for each sample (= for each instance, = for each row of y_true and y_pred), and then averaged across all samples.\n",
    "- per label : metrics are computed for each label separately, and then averaged across all labels.\n",
    "- per batch : metrics are computed for each batch, and then averaged across all batches.\n",
    "\n",
    "- 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "           Gives more weight to frequent labels → best for imbalanced datasets where frequent labels are more important.\n",
    "- 'macro': Calculate metrics for each label separately, and find their unweighted mean.\n",
    "           This does not take label imbalance into account. This is fine for balanced datasets but not for imbalanced datasets since rare labels are given equal weights.\n",
    "           Averages the metric for each label without considering their imbalance, without considering label frequency.\n",
    "           When to use: when wanting equal importance for all labels, including rare ones.\n",
    "           Treats all labels equally → best when you care about rare labels as much as frequent ones.\n",
    "- 'weighted': Calculate metrics for each label separately, and find their average weighted by support (= the number of true instances for each label). This alters ‘macro’ to account for label imbalance;\n",
    "              it can result in an F-score that is not between precision and recall.\n",
    "              Averages per label weighted by their support, without considering label frequency.\n",
    "              When to use: when wanting to reflect label imbalance (common labels contribute more).\n",
    "              Like macro but considers label frequency → best if you want a compromise between macro and micro.\n",
    "- 'samples': Calculate metrics per sample instead of per label, and find their average (only meaningful for multilabel-classification where this differs from accuracy_score).\n",
    "             Computes the metric per sample and then averages across all samples.\n",
    "             When to use: when each sample has multiple correct labels.\n",
    "\n",
    "- 'macro' or 'weighted' AUC is often best because AUC isn't as affected by class imbalance as F1/Precision/Recall\n",
    "- 'macro'      AUC: usually the best because it treats all labels equally, avoiding the dominance of frequent labels\n",
    "- 'weighted'   AUC: similar to macro but considers label frequency\n",
    "- 'macro'   PR AUC: best for imbalanced datasets because it treats rare labels fairly\n",
    "- 'weighted PR AUC: also good, but slightly biased toward frequent labels\n",
    "\n",
    "PR AUC is better than ROC AUC when you care about positive examples in imbalanced data.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBSQ6pLkMmhI"
   },
   "outputs": [],
   "source": [
    "training_average   = 'micro'             # 'weighted' (best) or 'samples\n",
    "evaluation_average = 'micro'             # 'macro'    (best) or 'weighted'\n",
    "test_average       = evaluation_average  #\n",
    "prediction_average = 'micro'             # 'micro'    (best) or 'samples' (prediction of unseen datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgxRu9m4Kb4_"
   },
   "source": [
    "## Tune thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KUJss-mKmRw"
   },
   "outputs": [],
   "source": [
    "threshold_tuning = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBe6WmEbwq2W"
   },
   "source": [
    "## Upload to HF Hub?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riGK-dwYr1kw"
   },
   "outputs": [],
   "source": [
    "upload_to_HF = True\n",
    "repo_id      = ''\n",
    "timestamp    = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98L73sydik95"
   },
   "source": [
    "## Hugging Face Hub (HF Hub) authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNxYo9txwcJs",
    "outputId": "a7169e3b-1fd7-48a7-9e82-cfa2c869e8f7"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "  os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")    # Store the key in os.environ\n",
    "  hf_token               = os.environ.get('HF_TOKEN')\n",
    "\n",
    "  login(token=hf_token)\n",
    "\n",
    "  # Check\n",
    "  user = whoami(token=hf_token)\n",
    "  assert user['name'] == 'claudelepere', f\"{user['name']} is not claudelepere\"\n",
    "  print(f\"user: {user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPBlVmE3wcJv"
   },
   "source": [
    "## repo_id, branch, model and dataset repos on HF Hub\n",
    "**1 repo = 1 model and 1 tokenizer**\n",
    "\n",
    "**branch = revision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1haRyaUwcJv",
    "outputId": "f2bb1d51-d74e-4765-f949-e8f904e53a2f"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "  if threshold_tuning:\n",
    "    repo_id   = 'claudelepere/jobs_EN_791112_50000_040511'\n",
    "  else:\n",
    "    repo_id   = 'claudelepere/jobs_EN_791112_50000_040511_no_threshold_tuning'\n",
    "\n",
    "  timestamp = f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "  model_repoUrl   = create_repo(repo_id=repo_id, repo_type=\"model\",   private=True, exist_ok=True)\n",
    "  dataset_repoUrl = create_repo(repo_id=repo_id, repo_type=\"dataset\", private=True, exist_ok=True)\n",
    "\n",
    "  #create_branch(repo_id=repo_id, repo_type=\"model\",   branch=branch, exist_ok=True)\n",
    "  #create_branch(repo_id=repo_id, repo_type=\"dataset\", branch=branch, exist_ok=True)\n",
    "\n",
    "  print(f\"Model Repo Url: {model_repoUrl} created successfully as a private repo\")\n",
    "  print(f\"Dataset Repo Url: {dataset_repoUrl} created successfully as a private repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1Ct9Ki6fLLL"
   },
   "source": [
    "## HF model card\n",
    "Model card here => README.md on the HF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8Wn7JfxfFI0"
   },
   "outputs": [],
   "source": [
    "model_card = \"\"\"\n",
    "---\n",
    "tags:\n",
    "- '50000'\n",
    "---\n",
    "# Model\n",
    "Model fine-tuned on higly imbalanced multilabel classification.\n",
    "\n",
    "## Model details\n",
    "- Language: English\n",
    "- Task: Multilabel classification\n",
    "- Architecture: Longformer\n",
    "- Pretrained model: [allenai/longformer-base-4096](https://huggingface.co/allenai/longformer-base-4096)\n",
    "- Framework: Pytorch\n",
    "- Version 1.0.0\n",
    "\n",
    "## Training Data\n",
    "- skills: 7, 9, 11, 12\n",
    "- 50000 job datapoints\n",
    "\n",
    "## Fine-tuning parameters\n",
    "- batch size: 8\n",
    "- gradient accumulation: 4\n",
    "- fp16 precision\n",
    "- input tokens max length: 1024\n",
    "- epochs: 5\n",
    "- learning rate: 1e-5\n",
    "- attention window size: 1024\n",
    "- training average: micro\n",
    "- evaluation average: micro\n",
    "- test average: micro\n",
    "- prediction average: micro\n",
    "- threshold tuning: True\n",
    "- threshold: 0.5\n",
    "- focal loss: alpha=0.5, gamma=4.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l8mrGaAqt3a"
   },
   "source": [
    "## Save locally and upload model card to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aY4jxVl1qnEy",
    "outputId": "3a3de50a-e366-4105-9492-1b92dfe4b765"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "    name            = \"model_card\"\n",
    "    model_card_path = f\"{name}.md\"\n",
    "\n",
    "    with open(model_card_path, \"w\") as f:\n",
    "        f.write(model_card)\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = model_card_path,\n",
    "        path_in_repo    = 'README.md',\n",
    "        repo_id         = repo_id,\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} results successfully uploaded to HF Hub as {model_card_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhAd6EZJwcJt",
    "outputId": "b33c76f7-ae84-4c74-98f2-47d63833aea0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Check the Python version\n",
    "print(sys.version)\n",
    "print()\n",
    "\n",
    "# Get the installed packages (you can see that conda is not installed (do not install it))\n",
    "!pip list\n",
    "print()\n",
    "\n",
    "# Check system information\n",
    "!cat /etc/os-release\n",
    "!uname -m\n",
    "print()\n",
    "\n",
    "# Check the GPU details (only if the runtime type is T4 GPU)\n",
    "#!nvidia-smi\n",
    "#print()\n",
    "\n",
    "# Check RAM\n",
    "!free -h\n",
    "print()\n",
    "\n",
    "# Check disk space\n",
    "!df -h\n",
    "print()\n",
    "\n",
    "# Get environment variables\n",
    "for key, value in os.environ.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\"\"\"\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUTMN8SSwcJt",
    "outputId": "e1617b67-ee88-4b32-b27b-a109f61ef7e2"
   },
   "outputs": [],
   "source": [
    "print(f\"currentdir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIUfehpkwcJt",
    "outputId": "2051d9e5-46c7-4e49-8b05-79e639a331f0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr4xZXGSXsEx"
   },
   "source": [
    "## Out Of Memory (OOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvigdvJgp8V9"
   },
   "source": [
    "### OOM: reduce batch size\n",
    "      small sizes (1 to 32):            PROs: better generalization in some cases\n",
    "                                        CONs: may produce noisier gradients\n",
    "      large sizes (128, 256, or higer): PROs: gradients are smoother, leading to more stable training\n",
    "                                        CONs: poorer generalization (overfitting) in some cases\n",
    "      intermediate sizes (32, 64):      combines the benefits of small and large sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq0JDT07wcJt"
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPERKMpOqa8X"
   },
   "source": [
    "### OOM: enable gradient accumulation\n",
    "\n",
    "* compensate for smaller batch sizes by accumulating gradients over several steps\n",
    "* **effective batch size** = per-device batch size x gradient acumulation steps\n",
    "* in each iteration, the model computes the gradients, these gradients are immediately used to update the model parameters\n",
    "\n",
    "WARNING: gradient_accumulation_steps may not be None => comment it in TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG0pflXGwcJt"
   },
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yOaxNG3rzUt"
   },
   "source": [
    "### OOM: use PYTORCH_CUDA_ALLOC_CONF to handle memory fragmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXrPvSWAwcJt"
   },
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmIswaWGsBBV"
   },
   "source": [
    "### OOM: check for and kill zombie processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKeGyFYHwcJu",
    "outputId": "a4d66b0f-f6f7-4f05-af28-c32326e275ca"
   },
   "outputs": [],
   "source": [
    "!ps aux | grep python\n",
    "!kill -9 <PID>\n",
    "if torch.cuda.is_available():\n",
    "  !nvidia-smi\n",
    "  print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpKIEUjjsO-i"
   },
   "source": [
    "### OOM: use fp16 (half precision) mixed precision training\n",
    "reduces memory requirements by up to 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbyMivrEwcJu"
   },
   "outputs": [],
   "source": [
    "fp = 'fp16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVhoxNtUwcJu"
   },
   "source": [
    "### OOM: limit the number of GPU workers:\n",
    "* 0 (default) or 1\n",
    "* in Colab dataloader_num_workers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-elhl5Pkssq8"
   },
   "source": [
    "### OOM: reduce model size or input tokens\n",
    "* LongformerTokenizer.from_pretrained('allenai/longformer-base/large-4096'): large/base: 435M/149M parameters\n",
    "* max_length: 4096 max for Longformer\n",
    "* a single word can be equal to several tokens; stop words are **NOT discarded**!\n",
    "* word_text_length_counts_sorted:\n",
    "      jobs count                 : 50000\n",
    "      jobs count under  512 words: 44794  89.59%\n",
    "      jobs count under  640 words: 47894  95.79%\n",
    "      jobs count under  768 words: 49123  98.25%\n",
    "      jobs count under  896 words: 49691  99.38%\n",
    "      jobs count under 1024 words: 49917  99.83%\n",
    "      jobs count under 2048 words: 50000 100.00%\n",
    "      jobs count under 4096 words: 50000 100.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZOpDR5JwcJu"
   },
   "outputs": [],
   "source": [
    "#max_length =  768    #      37 min    #\n",
    "max_length = 1024    #      38 min    # GPU RAM: 12.2 / 40 GB\n",
    "#max_length = 2048    # 1 hr 10 min    # GPU RAM: 21.4 / 40 GB\n",
    "#max_length = 4096    # 2 hr 10 min    # GPU RAM: 39.5 / 40 GB => OutOfMemoryError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4v9XcFLuudV"
   },
   "source": [
    "### OOM: free up GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJNB5_2HwcJu"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVi2_ymCwcJu"
   },
   "source": [
    "### OOM: reduce the number of transformers layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBu-UPkq0Rq0"
   },
   "outputs": [],
   "source": [
    "# hidden_layers = 6  # default:12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qizOQRXNu54F"
   },
   "source": [
    "## epoch\n",
    "* 1 epoch is a complete pass through the entire training dataset\n",
    "* with n datapoints and batch size = b, n/b iterations to complete 1 epoch\n",
    "* 1 iteration is a single update of the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KORJtBl7wcJu"
   },
   "outputs": [],
   "source": [
    "#epochs = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBRPtOsqvm2P"
   },
   "source": [
    "## learning rate\n",
    "* A common rule is to scale the learning rate proportionaly with the effective batch size\n",
    "* **note: get_linear_schedule_with_warmup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaoQWRXYwcJu"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5 #1e-5  # 1e-5 x 32/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1j87eP0wCR7"
   },
   "source": [
    "## threshold\n",
    "default: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2Dfm6lvwcJu"
   },
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNDt1EVUwOWE"
   },
   "source": [
    "## attention window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-CmoMf9wcJ0"
   },
   "outputs": [],
   "source": [
    "attention_window = 1024 #512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPt8yoHMsksM"
   },
   "source": [
    "## Upload and unzip job dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "ggSY4TBWXfvS",
    "outputId": "e48bfb21-97e7-4667-b633-972b7ee3d4bb"
   },
   "outputs": [],
   "source": [
    "def upload_unzip_dataset(filename):\n",
    "    \"\"\"Upload and unzip the dataset to /content, ensuring correct placement.\"\"\"\n",
    "\n",
    "    # Get the expected directory name (same as the zip filename without extension)\n",
    "    expected_dir = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Check if the file and the directory exist in /content and delete them\n",
    "    with suppress(FileNotFoundError):\n",
    "        if os.path.isdir(expected_dir):\n",
    "            shutil.rmtree(expected_dir)  # Remove directory if it exists\n",
    "        if os.path.isfile(filename):\n",
    "            os.remove(filename)          # Remove file if it exists\n",
    "\n",
    "    print(f\"Removed '{expected_dir}' and '{filename}' if they were present in /content.\")\n",
    "\n",
    "    # Upload the zip file\n",
    "    uploaded_files = files.upload()  # Prompt file upload dialog\n",
    "\n",
    "    if filename not in uploaded_files:\n",
    "        raise FileNotFoundError(f\"'{filename}' was not uploaded.\")\n",
    "\n",
    "    print(f\"'{filename}' successfully uploaded to /content.\")\n",
    "\n",
    "    # Unzip the file to /content\n",
    "    shutil.unpack_archive(filename, \"/content\")\n",
    "\n",
    "    print(f\"Unzipped to '/content/{expected_dir}'.\")\n",
    "\n",
    "# Usage\n",
    "upload_unzip_dataset(datasetDict_zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T90_cKa4wcJv"
   },
   "source": [
    "## W&B initialization (not used now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gweGUl--FNsZ",
    "outputId": "c333e4fe-7bac-4a88-b89f-e147ff626c32"
   },
   "outputs": [],
   "source": [
    "run_name = f\"EN_{skills}_{all_rows_low}_{all_rows_high}_ml{max_length}_ep{epochs}_lr{learning_rate}_th{threshold}_at{attention_window}_{fp}\"\n",
    "\n",
    "if 'gradient_accumulation_steps' not in globals():\n",
    "  run_name = f\"{run_name}_ba{batch_size}\"\n",
    "else:\n",
    "  run_name = f\"{run_name}_ba{batch_size}x{gradient_accumulation_steps}\"\n",
    "\n",
    "print(f\"run_name: {run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giGsdqlUwcJz",
    "outputId": "d59deab2-2909-47db-9cf1-f99e2dfc16fb"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")        # Store the key in os.environ\n",
    "wandb_api_key               = os.environ.get('WANDB_API_KEY')\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "JYJMUPCONEiD",
    "outputId": "eecaef2a-5d87-4c4a-a81a-41a14e93c06a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  wandb.init(\n",
    "      project = \"skill_classification\",\n",
    "      name    = run_name,\n",
    "      entity  = \"claudelepere-c-cile-cy\",\n",
    "      config  = {\n",
    "          \"learning_rate\": learning_rate,\n",
    "          \"epochs\"       : epochs,\n",
    "          \"batch_size\"   : batch_size\n",
    "      }\n",
    "  )\n",
    "except wandb.CommError as err:\n",
    "  print(f\"CommError: {err}\")\n",
    "except Exception as exc:\n",
    "  print(f\"Exception: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfBhdTN9wcJz"
   },
   "source": [
    "## Create datasetDict (HF DatasetDict) = 3 HF Dataset, train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh_SgQD9wcJz"
   },
   "outputs": [],
   "source": [
    "datasetDict = DatasetDict.load_from_disk(datasetDict_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcF4Gm8GFNsc",
    "outputId": "49682660-1c61-43c3-d229-d19ff8e7dfb5"
   },
   "outputs": [],
   "source": [
    "print(f\"datasetDict: {type(datasetDict)} {datasetDict.shape}\\n{datasetDict}\")\n",
    "print(f\"datasetDict.keys(): {datasetDict.keys()}\")\n",
    "print(f\"datasetDict['train']:      {type(datasetDict['train'])}      {datasetDict['train'].shape}\")\n",
    "print(f\"datasetDict['validation']: {type(datasetDict['validation'])} {datasetDict['validation'].shape}\")\n",
    "print(f\"datasetDict['test']:       {type(datasetDict['test'])}       {datasetDict['test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unjuTtKUjZI3",
    "outputId": "6166057d-57d9-49e6-cbb5-816bf2a08bdf"
   },
   "outputs": [],
   "source": [
    "example = datasetDict['train'][0]\n",
    "print(f\"datasetDict['train'][0]: {type(example)} {example.keys()}\\n{example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEgARWcywcJz"
   },
   "source": [
    "## Create labels (list), id2label (dict) and label2id (dict).\n",
    "* dataset 7_1000_125_125  ,  48 labels\n",
    "* dataset 7_128_18_54     ,  42 labels\n",
    "* dataset 8910_1087_68_204, 206 labels\n",
    "* dataset 11_1000         ,   6 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vv6FWVzRwcJz",
    "outputId": "9c561c67-e533-428a-852d-1b8b88480f45"
   },
   "outputs": [],
   "source": [
    "labels = [label for label in datasetDict['train'].features.keys() if label not in ['id', 'text']]\n",
    "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")\n",
    "\n",
    "num_labels = len(labels)\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "print(f\"id2label: {type(id2label)} {len(id2label)}\\n{id2label}\")\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "print(f\"label2id: {type(label2id)} {len(label2id)}\\n{label2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_TdBZOGwcJ0"
   },
   "source": [
    "## Load the pretrained tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL1IlzvfwcJ0"
   },
   "outputs": [],
   "source": [
    "model_name = \"allenai/longformer-base-4096\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "3e89e21097a44e23b607a21bb6a8abc8",
      "786c9a6873e24dcca49225637abd3c7f",
      "11595c0b69794a73b5467cb7a7b970a2",
      "517b0b1221294f188c5b6298bb60a001",
      "40333ea699db46338142c9813df77d27",
      "e163b90589f946abb11fc5cc3f70b343",
      "5c802c1c741a4ba099b2395b3441c858",
      "057500a058af48ecb56b9300d5a10fcd",
      "c35713d961c64348806239d5f8d08470",
      "0951d4eea2c144b1a269651f83f933c6",
      "df229ffd2c464f69953c752bfbb3e97d",
      "f9f51d15c1f340edac2c5c9bd4b5e29b",
      "194e67a42da846e7a58c9ba4198208eb",
      "74410e8cf6d04bed9bfdeb0e2badb426",
      "288aad6a9f894db894e66aac2f40d2b6",
      "4cf47cb784ce4601925c0a7f41d6f925",
      "6b364dbffd85423abf9e16f51168587e",
      "8f284c0d2c8f4152bad0b8d78f4afe42",
      "7c5330bbdba243b4a34b506d8c1c757b",
      "7fdb857491c04a60a2c9ab68a2028643",
      "f444c96245f1429685443d65c5c78d85",
      "d15ba33d54f5418380e1319e9b26b91c",
      "36c53a9f2b6c4f40ba65a0b5a2b47549",
      "11c8ec508cd4494bb75df5f2eec4a58c",
      "e66290bc09294ec0be90d2127010eeec",
      "d4068c96945f4a819a2f14c74fbf57a1",
      "a59803967834417ab6a27c0ffddf3de9",
      "bf6a90b5b3e241d0b17c61e2ec6838c8",
      "83256a5a81d34568a7fa8095d52a047c",
      "90c183c7b0a341638359afaa5821013c",
      "16b88876fae7402e8f496db641caeeb5",
      "9c431dfea79441ff8d6f04d45257ce02",
      "09514272862a4f9cb4ff69cf33d6fc9e",
      "22d18fcc11a64bf2bb49455b66c2c4e7",
      "5a5898679f044a13b2c8ba49ac1b41f2",
      "c09a6d7eb136412fada632f9921d70a4",
      "551cc7f837344a73b5001357b184f1c2",
      "ffd0b28e8ed04d0e8c8bcb925b89551a",
      "2a7fc603eeca493e80a1d94d4587e0fc",
      "b67b1b056a7b45b1b2dd57df63bbd9c6",
      "3c14b011a5be4525976316bf233f2251",
      "753eeea2766042e3a7163c1cde1daf43",
      "2f9f1c62336949438fd112f13f5442ef",
      "691fe41762694b059521c17d151679a8"
     ]
    },
    "id": "mwKcOR8GwcJ0",
    "outputId": "a7817e22-ffd3-430c-d823-72a8f076f0bc"
   },
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "86008dd6f4e343dab468ac105ddeb512",
      "3e9d6882cd0747abb0a728dfe0cd67b9",
      "b00f4bcb0bf44c2dacb915684bbd0973",
      "0c1c0f40de61438b9f448ffbfc415524",
      "baacce94efab4f719b150b4390040309",
      "c3f0d818df3e4d7e955f37b712e63ac3",
      "b305b8e012cd448b90d06f6c1d0a0577",
      "96c93c571205482993b9d99909c51247",
      "ea11c6a21d7c4c73be1cc49cf191b370",
      "a963739e3c02405c952908bdd42d2e7d",
      "5163289a45a0457289c47ab7940aaaa6"
     ]
    },
    "id": "HTZ61-RUwcJ0",
    "outputId": "0b1b9ca8-78aa-4f4f-d53d-b17bbb101599"
   },
   "outputs": [],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels   = num_labels,\n",
    "    id2label     = id2label,\n",
    "    label2id     = label2id,\n",
    "    problem_type = 'multi_label_classification'\n",
    ")\n",
    "\n",
    "# Configure attention window size\n",
    "model.config.attention_window = attention_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjLO31SssqAM"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTYt0hM5wcJ0"
   },
   "source": [
    "## Tokenize ('input_ids' and 'attention_mask'), add 'global_attention_mask' (for Longformer), add 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFWlSsbZaRLc"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(examples, indices):\n",
    "  # Step 1: Extract text and tokenize\n",
    "  text = examples['text']             # Batch of texts\n",
    "  encoding = tokenizer(\n",
    "      text,                           # Tokenize text\n",
    "      truncation     = True,\n",
    "      padding        = 'max_length',\n",
    "      max_length     = max_length,\n",
    "      return_tensors = 'pt'           # Return PyTorch tensors\n",
    "  )\n",
    "\n",
    "  # Step 2: Create and add the global attention mask\n",
    "  global_attention_mask             = torch.zeros_like(encoding['input_ids'])  # Initialize global attention mask with zeros (same shape as input_ids)\n",
    "  global_attention_mask[:, 0]       = 1                                        # Set global attention on the first token ([CLS], token ID=0) in each sequence\n",
    "  encoding['global_attention_mask'] = global_attention_mask                    # Add the global_attention_mask to the batch\n",
    "\n",
    "  # Step 3: Create and populate the label matrix\n",
    "  labels_matrix = torch.zeros((len(text), len(labels)), dtype=torch.float32)   # Create an empty label matrix\n",
    "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
    "  #---------Populate label matrix\n",
    "  for idx, label in enumerate(labels):\n",
    "    #print(f\"idx:{idx} label:{label}\")\n",
    "    if label in examples:\n",
    "      labels_matrix[:, idx] = torch.tensor(\n",
    "          [1.0 if val else 0.0 for val in examples[label]],\n",
    "          dtype=torch.float32\n",
    "          )\n",
    "  print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
    "\n",
    "  encoding['labels'] = labels_matrix                                           # Add labels to the encoding\n",
    "  print(f\"encoding['labels']: {type(encoding['labels'])} {encoding['labels'].shape}\")\n",
    "\n",
    "  # encoding: <class 'transformers.tokenization_utils_base.BatchEncoding'> dict_keys(['input_ids', 'attention_mask', 'global_attention_mask', 'labels'])\n",
    "  #   'input_ids': tensor([[\n",
    "  #   'attention_mask': tensor([[\n",
    "  #   'global_attention_mask': tensor([[\n",
    "  #   'labels': tensor([[\n",
    "  #print(f\"1 preprocess_data call: encoding: {type(encoding)} {encoding.keys()}\")\n",
    "\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm9REDT_wcJ0"
   },
   "source": [
    "## Create encoded_dataset (datasets.dataset_dict.DatasetDict) = 3 encoded datasets.arrow_dataset.Dataset, train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c2c2e822d6fb4a74990b31f4cab9519f",
      "0bde6840772d4654b663d4ea4dd65215",
      "c2d9d087a2d54175810f315a9c443b82",
      "9089419f11af4c48bb0455931d37769c",
      "7e25f31b926b47a78655b2d72d93be93",
      "59ea41cdfd714cb7acc3626838b05c73",
      "e13fb82d14af4527ab915e33d7e45c39",
      "9adf0d3edd044e21b5e7c5d623ef6e0a",
      "ff48cd016b274d32a33108d1e96ad62c",
      "8bb1020314ab48c591a66f51e0bc5a59",
      "7505451e820642f7a3086ff07a074899",
      "f59895f40e504e3287c9c3edb95b3e8b",
      "c01a6005d69d4263a41528a7afe877ef",
      "0b1822aba7bf40eea40b8130b865d008",
      "e2350272fc86470285045876125ee8bd",
      "fa5b6e3fca1d4dbf9c36ac6c2d7758e2",
      "eb8d4d150cb149abaedfc8dac32d91e7",
      "4ff5ce3034fe4b5eb60fbe5d44901b1d",
      "4f0afa26fd16440db129ea9268165880",
      "031775379b1445198215c9a58eae1519",
      "eab9d3e6a1e34d7f92e61450ba956f52",
      "4b16e37cfe1d467fbc0d384bdb793e62",
      "372fbeb0cd854fa08c1283b165192abb",
      "9777476144a345489e59d74a87fb6ca5",
      "33c5920ecdd945a5b2111d74a8cb34e0",
      "6833bd2366644d41b33a1509f3f3c816",
      "98b46777d80e4e7fab909773172ff84e",
      "709c8b5dd1584c18a196956fe5bdd837",
      "3f7061a2177d47e79d98f146851cbf18",
      "8518e910bf834c20a918de30485c01a1",
      "f10d48eaaa464446bf4b1337d2e07622",
      "bba8840361484c32a9d80bfbf09385fe",
      "3b299da1b2024b5196a5a70e262e6e10",
      "8cb06cd45992463ab56300bbde75e489",
      "832e81201aef4ec48df07e05451d85b7",
      "7e3f9c1797ee452c9a0a1703ccf4b9ee",
      "2d0858a068a141788a8e67ea9894aa1d",
      "8e4ee67937fd4216b02765268778436a",
      "f116d41e09d1487081b188db6e9cf7b8",
      "841bd52c49e44f90b5dbf2aa0b2dc646",
      "409d60d71b094788beaa38e788a86e86",
      "a6d2f8f40f1d45dab115f1d667e87b32",
      "e8c66631c0c64bef9e1b7013e2e49bf2",
      "3327f9a2d9704c8abf949cd0acf198a4"
     ]
    },
    "id": "mxLcO0XDwcJ0",
    "outputId": "f7a19985-a842-4477-8909-3bdc210b0aff"
   },
   "outputs": [],
   "source": [
    "encoded_dataset = datasetDict.map(\n",
    "    preprocess_data,\n",
    "    batched        = True,\n",
    "    remove_columns = datasetDict['train'].column_names,\n",
    "    with_indices   = True\n",
    ")\n",
    "\n",
    "print(f\"encoded_dataset: {type(encoded_dataset)} shape={encoded_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuft8rJe2Q03",
    "outputId": "7bd8c31f-2a9b-42f9-c846-f62db8b9f51c"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format('torch')\n",
    "train_dataset      = encoded_dataset['train']\n",
    "validation_dataset = encoded_dataset['validation']\n",
    "test_dataset       = encoded_dataset['test']\n",
    "\n",
    "print(f\"train_dataset_tensor:                          {type(train_dataset)}                              {train_dataset.shape} {train_dataset.features}\\n{train_dataset}\")\n",
    "print(f\"train_dataset_tensor['input_ids']:             {type(train_dataset['input_ids'])}             len={len(train_dataset['input_ids'])}             shape={train_dataset['input_ids'].shape}            \") #\\n{train_dataset['input_ids']}\")\n",
    "print(f\"train_dataset_tensor['attention_mask']:        {type(train_dataset['attention_mask'])}        len={len(train_dataset['attention_mask'])}        shape={train_dataset['attention_mask'].shape}       \") #\\n{train_dataset['attention_mask']}\")\n",
    "print(f\"train_dataset_tensor['global_attention_mask']: {type(train_dataset['global_attention_mask'])} len={len(train_dataset['global_attention_mask'])} shape={train_dataset['global_attention_mask'].shape}\") #\\n{train_dataset['global_attention_mask']}\")\n",
    "print(f\"train_dataset_tensor['labels']:                {type(train_dataset['labels'])}                len={len(train_dataset['labels'])}                shape={train_dataset['labels'].shape}               \") #\\n{train_dataset['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynwZnA55wcJ1"
   },
   "source": [
    "## Truncated part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brkRdqdjN-Ur"
   },
   "outputs": [],
   "source": [
    "def get_truncated_part(text):\n",
    "  tokens = tokenizer(\n",
    "      text,\n",
    "      truncation                = True,\n",
    "      padding                   = 'max_length',\n",
    "      max_length                = max_length,\n",
    "      return_overflowing_tokens = True,\n",
    "      return_tensors            = None\n",
    "  )\n",
    "  print(f\"tokens.keys(): {tokens.keys()}\")\n",
    "\n",
    "  # Get the truncated tokens\n",
    "  truncated_ids = tokens[\"input_ids\"][0]\n",
    "  print(f\"truncated_ids: {type(truncated_ids)} {truncated_ids}\")\n",
    "  #overflow_ids  = tokens[\"overflow_to_sample_mapping\"][0]\n",
    "  #print(f\"overflow_ids: {type(overflow_ids)} {overflow_ids}\")\n",
    "\n",
    "  # Decode the tokens back to text\n",
    "  truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
    "  #overflow_text  = tokenizer.decode(overflow_ids, skip_special_tokens=True)\n",
    "\n",
    "  print(f\"original_text :\\n{text}\")\n",
    "  print(f\"truncated_text:\\n{truncated_text}\")\n",
    "  #print(f\"overflow_text:\\n{overflow_text}\")\n",
    "\n",
    "  original_tokens  = tokenizer.tokenize(text)\n",
    "  truncated_tokens = tokenizer.tokenize(truncated_text)\n",
    "  #overflow_tokens  = tokenizer.tokenize(overflow_text)\n",
    "\n",
    "  print(f\"original_tokens count : {len(original_tokens)}\")\n",
    "  print(f\"truncated_tokens count: {len(truncated_tokens)}\")\n",
    "  #print(f\"overflow_tokens count: {len(overflow_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD_bjwTfRiQO"
   },
   "outputs": [],
   "source": [
    "example_text = datasetDict['train'][0]['text']\n",
    "#get_truncated_part(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFiZ4mYmwcJ1"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    example_text,\n",
    "    truncation     = True,\n",
    "    padding        = 'max_length',\n",
    "    max_length     = max_length,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTh2E_8iwcJ1"
   },
   "source": [
    "## Forward pass for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_aIPMHuwcJ1",
    "outputId": "13211b5e-5e6f-43db-ff96-f4ce266d6ea7"
   },
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "    input_ids      = inputs.input_ids,\n",
    "    attention_mask = inputs.attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWcnZ8ku12V",
    "outputId": "8dfa695d-e576-497c-d929-309a0791a3d8"
   },
   "outputs": [],
   "source": [
    "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "islk-kFSs0t8",
    "outputId": "c003abb2-fdd1-47f7-966c-785eb635a32e"
   },
   "outputs": [],
   "source": [
    "# Logits (= raw model outputs)\n",
    "logits = outputs.logits\n",
    "print(f\"logits: {type(logits)} {logits.shape}\\n{logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMscqNTXuY8o",
    "outputId": "fa986359-e5db-46f0-c918-32de07d77fb7"
   },
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs   = sigmoid(logits)\n",
    "print(f\"probs: {type(probs)} {probs.shape}\\n{probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZtO3uQkwcJ2"
   },
   "outputs": [],
   "source": [
    "example = encoded_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0enAb0W9o25W",
    "outputId": "9f432259-e3e4-48f1-8b7a-b86f2ee2e731"
   },
   "outputs": [],
   "source": [
    "print(f\"example: {type(example)} {example.keys()}\\n{example}\")\n",
    "print()\n",
    "#print(f\"example['input_ids']: {type(example['input_ids'])} {len(example['input_ids'])}\\n{example['input_ids']}\")\n",
    "#print(f\"example['attention_mask']: {type(example['attention_mask'])} {len(example['attention_mask'])}\\n{example['attention_mask']}\")\n",
    "#print(f\"example['labels']:  {type(example['labels'])} {len(example['labels'])}\\n{example['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "D0McCtJ8HRJY",
    "outputId": "f052ddff-8aa4-400f-fa56-fdca65b78600"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LAyThO7Jnvj",
    "outputId": "87310127-3832-4b18-efdb-d689af09e927"
   },
   "outputs": [],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHCJcLKGwcJ2"
   },
   "source": [
    "## Set PyTorch format to ensures correctness and compatibility with PyTorch pipelines\n",
    "The 3 Hugging Face Dataset are formatted as PyTorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4ENBTdulBEI"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8aGgHN-BQrO"
   },
   "source": [
    "## Workflow\n",
    "\n",
    "- 3 steps: training, evaluation, test\n",
    "- 3 datasets: train, validation, test\n",
    "- 3 Trainer functions: train, evaluate, predict\n",
    "---\n",
    "* training uses train_dataset\n",
    "* evaluation uses validation_dataset\n",
    "* test uses test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy7jlzubwcJ6"
   },
   "source": [
    "## Training step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5a8_vIKqr7P"
   },
   "outputs": [],
   "source": [
    "batch_size  = batch_size\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adTwB7XvFNsj",
    "outputId": "3970dea5-407a-49a4-d315-0a7dcd6f7f00"
   },
   "outputs": [],
   "source": [
    "print(f\"input_ids:              {type(encoded_dataset['train']['input_ids'][0])}\\t{encoded_dataset['train']['input_ids'][0].shape}\")\n",
    "print(f\"attention_mask:         {type(encoded_dataset['train']['attention_mask'][0])}\\t{encoded_dataset['train']['attention_mask'][0].shape}\")\n",
    "print(f\"global_attention_mask:  {type(encoded_dataset['train']['global_attention_mask'][0])}\\t{encoded_dataset['train']['global_attention_mask'][0].shape}\")\n",
    "print(f\"labels:                 {type(encoded_dataset['train'][0]['labels'])}\\t{encoded_dataset['train'][0]['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCGdbfNJwcJ4"
   },
   "source": [
    "### Execute a forward pass for debugging or verification purposes (cf. BERT_3_1 in Notion BERT database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN-sEDggwcJ4"
   },
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "    input_ids      = encoded_dataset['train']['input_ids'][0].unsqueeze(0),\n",
    "    attention_mask = encoded_dataset['train']['attention_mask'][0].unsqueeze(0),\n",
    "    labels         = encoded_dataset['train'][0]['labels'].unsqueeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtEd6KmRwcJ4",
    "outputId": "b7c990d7-5d52-4e0f-a2aa-758132be230f"
   },
   "outputs": [],
   "source": [
    "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYiSTBw_1Kdj"
   },
   "source": [
    "### Weighted loss function\n",
    "**weight and pos_weight**:\n",
    "- torch.nn.BCEWithLogitsLoss function is a commonly used loss function for binary classification problems, where model output is a probability value between 0 and 1. It combines a sigmoid activation function with a binary cross-entropy loss.\n",
    "- For imbalanced datasets, where number of a class is significantly smaller than other, BCEWithLogitsLoss can be modified by adding a weight parameter to loss function.<br/>\n",
    "BCEWithLogitsLoss also has a pos_weight parameter, which is a simpler way to specify weight for positive class (equivalent to weight parameter = [ 1, pos_weight], where weight for negative class = 1.<br>\n",
    "Negative samples (0s) are not weighted explicitly because the loss function already balances them implicitly.\n",
    "- pos_weights stands for positive weights in the BCEWithLogitsLoss function.<br/>\n",
    "In multi-label classification, each label is a separate binary classification problem.<br/>\n",
    "Each label has positive sample (1s) and negative samples (0s) in the dataset.\n",
    "    - If a label is rare (fewer 1s), its weight will be higher -> encourages the model to predict it more often\n",
    "    - If a label is common (many 1s), its weight will be lower -> prevents the model to overpredicting it\n",
    "\n",
    "**Normalization**:\n",
    "Without normalization, pos_weights might have huge variations across labels, that could destabilize training.\n",
    "- Min-Max Scaling:\n",
    "    - Rescales values between 0 and 1\n",
    "    - Reduces large variations but keeps relative ranking\n",
    "- Z-Score Normalization:\n",
    "    - Centers values around 0 with a standard deviation of 1\n",
    "    - Handles outliers better than min-max\n",
    "- Sum-to-One Scaling:\n",
    "    - Makes weights sum to 1, preventing extremely large values\n",
    "- Recommended approach: try sum-to-one normalization first. If performance is unstable, test z-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d89HI1SjJNkO"
   },
   "source": [
    "### Weighted BCEWithLogitsLoss\n",
    "Assigns higher weights to rare labels using class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqMipSpowcJ5"
   },
   "outputs": [],
   "source": [
    "def class_weights(labels):\n",
    "    print(f\"labels: {type(labels)} len={len(labels)} shape={labels.shape}\\n{labels}\")\n",
    "\n",
    "    num_samples, num_labels = labels.shape\n",
    "    print(f\"num_samples: {type(num_samples)} {num_samples}\")\n",
    "    print(f\"num_labels:  {type(num_labels)}  {num_labels}\")\n",
    "\n",
    "    # class_counts = how many times each label appears (i.e. number of 1s per label)\n",
    "    # (dim=0 means summing across all samples; equivalent to axis = 0 for Pandas DataFrame)\n",
    "    class_counts = labels.sum(dim=0)\n",
    "    print(f\"class_counts: {type(class_counts)} len={len(class_counts)}\\n{class_counts}\")\n",
    "\n",
    "    # pos_weights = negative samples (0s) per label / positive samples (1s) per label\n",
    "    pos_weights = (num_samples-class_counts) / (class_counts + 1e-6)  # Avoid division by zero\n",
    "    print(f\"pos_weights: {type(pos_weights)} len={len(pos_weights)}\\n{pos_weights}\")\n",
    "\n",
    "    # Normalization\n",
    "    normalized_pos_weights_minmax = (pos_weights-pos_weights.min()) / (pos_weights.max()-pos_weights.min())\n",
    "    print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
    "\n",
    "    normalized_pos_weights_zscore = (pos_weights-pos_weights.mean()) / pos_weights.std()\n",
    "    print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
    "\n",
    "    normalized_pos_weights_sum1 = pos_weights / pos_weights.sum()\n",
    "    print(f\"normalized_pos_weights_sum1: {type(normalized_pos_weights_sum1)} {len(normalized_pos_weights_sum1)} {normalized_pos_weights_sum1}\")\n",
    "\n",
    "    #return pos_weights\n",
    "    #return normalized_pos_weights_minmax\n",
    "    #return normalized_pos_weights_zscore\n",
    "    return normalized_pos_weights_sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Bh-gAHGwcJ5",
    "outputId": "b81e3d64-30a4-43fb-bf59-d02f7159c13d"
   },
   "outputs": [],
   "source": [
    "pos_weights = class_weights(encoded_dataset['train']['labels'])\n",
    "bce_loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_lLw3Er1WGj"
   },
   "source": [
    "### Focal Loss\n",
    "Reduces the impact of easy examples (majority class) and focuses on difficult cases.\n",
    "\n",
    "- α (alpha): Adjusts class weighting (0.5 means equal weight). Higher α gives more weight to minority classes.\n",
    "- γ (gamma): Controls how much hard-to-classify samples are emphasized. Higher γ reduces the influence of easy samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPRpdL8fwcJ5"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(Module):\n",
    "    \"\"\"\n",
    "    Focal Loss implementation for handling class imbalance.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, logits=True, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha     = alpha\n",
    "        self.gamma     = gamma\n",
    "        self.logits    = logits     # True if inputs are logits, False if probabilies\n",
    "        self.reduction = reduction  # 'mean' or 'none'\n",
    "\n",
    "    # inputs  = model's predictions: PyTorch tensor, shape=(batch_size, num_classes)\n",
    "    # targets = ground truth labels: PyTorch tensor, shape=same as inputs shape\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.to(inputs.device)  # Ensure labels are on the same device\n",
    "\n",
    "        #print(f\"inputs: {type(inputs)} {inputs.shape}\\ntargets: {type(targets)} {targets.shape}\"\n",
    "        # Here, we check if input is probability or logits\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "\n",
    "        pt         = torch.exp(-BCE_loss)  # Probability of the correct class\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FocalLoss(alpha={self.alpha}, gamma={self.gamma}, logits={self.logits}, reduction={self.reduction})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zknwQWsEb7oP",
    "outputId": "102844f9-d221-4deb-8ecb-83253fe55e55"
   },
   "outputs": [],
   "source": [
    "#focal_loss_fn = FocalLoss(alpha=0.5, gamma=3.0, logits=True, reduction='mean')\n",
    "#focal_loss_fn = FocalLoss(alpha=0.25, gamma=4.0, logits=True, reduction='mean')\n",
    "focal_loss_fn = FocalLoss(alpha=0.5, gamma=4.0, logits=True, reduction='mean')\n",
    "#focal_loss_fn = FocalLoss(alpha=0.625, gamma=4.0, logits=True, reduction='mean')\n",
    "print(f\"focal_loss_fn: {type(focal_loss_fn)} {focal_loss_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du0yNyrN2xad"
   },
   "outputs": [],
   "source": [
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Retrieve loss from logs\n",
    "        logs       = kwargs.get(\"logs\", {})\n",
    "        epoch_loss = logs.get(\"logs\", None)  # Loss from the Trainer logs\n",
    "\n",
    "        if epoch_loss is not None:\n",
    "            logging.info(f\"Epoch {state.epoch:.0f} - Average Loss: {epoch_loss:.6f}\")\n",
    "            #print(f\"Epoch {state.epoch:.0f} - Average Loss: {epoch_loss:.6f}\", flush=True)\n",
    "        else:\n",
    "            logging.warning(f\"Epoch {state.epoch:.0f} - No loss logged!\")\n",
    "            #print(f\"Epoch {state.epoch:.0f} - No loss logged!\", flush=True)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFUPLYtS5Aik"
   },
   "outputs": [],
   "source": [
    "class MetricsLoggerCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is None:\n",
    "            metrics = kwargs.get(\"metrics\")  # Ensure we get the metrics if passed in kwargs\n",
    "        if metrics:                          # Check if metrics exist\n",
    "            logging.info(f\"Epoch {state.epoch:.0f} - \"\n",
    "                         f\"Precision: {metrics.get('precision', float('nan')):.4f} - \"\n",
    "                         f\"Recall: {metrics.get('recall', float('nan')):.4f} - \"\n",
    "                         f\"F1: {metrics.get('f1', float('nan')):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obiPikg4UMk9"
   },
   "outputs": [],
   "source": [
    "class ProgressLoggerCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        logs       = kwargs.get(\"logs\", {})  # Extract from logs\n",
    "        epoch_loss = logs.get(\"loss\", None)  # Get loss value\n",
    "\n",
    "        if epoch_loss is not None:\n",
    "            logging.info(f\"Epoch {state.epoch:.0f} - Average Loss: {epoch_loss:.6f}\")\n",
    "            #print(f\"Epoch {state.epoch:.0f} - Average Loss: {epoch_loss:.6f}\", flush=True)\n",
    "        else:\n",
    "            logging.warning(f\"Epoch {state.epoch:.0f} - No loss logged!\")\n",
    "            #rint(f\"Epoch {state.epoch:.0f} - No loss logged!\", flush=True)\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        logging.info(\"Training Completed!\")\n",
    "        #print(\"=== Training Completed! ===\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UVmqyLTDcBQ"
   },
   "source": [
    "### Training Metrics\n",
    "  source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZleLBD1NXyo"
   },
   "source": [
    "#### UndefinedMetricWarning\n",
    "Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
    "\n",
    "This warning typically arises when you're trying to calculate the ROC AUC score for a label where either all true values are 0 or all are 1 in a particular batch.\n",
    "\n",
    "The ROC AUC score is calculated by comparing the true positive rate (TPR) against the false positive rate (FPR) at various thresholds. If a label is only ever predicted as 0 or 1, you cannot generate a meaningful ROC curve and thus the AUC is undefined.\n",
    "\n",
    "AUC scores rely on the presence of both positive and negative samples for each label.\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Check Label Distribution: Add a check at the start to see if either true_labels or preds for a particular label contain only one unique value (0 or 1). If so, for that label, either skip the ROC AUC calculation or set the ROC AUC to a default value (like 0 or NaN).\n",
    "- Ignore the warning (not recommended):\n",
    "- **Stratified sampling: While you did split into train/validation/test, the warning may indicate you didn't maintain the label balance during the splitting process. Stratified sampling would do this.**\n",
    "\n",
    "#### TPR (True Positive Rate) and FPR (False Positive Rate)\n",
    "TPR = Sensitivity = Recall = TP / (TP + FN)\n",
    "- TPR close to 1: the model identifies most positives\n",
    "- TPR close to 0: the model is missing many positives\n",
    "\n",
    "FPR = FP / (FP + TN)\n",
    "- FPR close to 1: the model produces many false alarms\n",
    "- FPR close to 0: the model makes few false alarms\n",
    "\n",
    "The ROC curve plots TPR (y-axis) vs. FPR (x-axis) at different thresholds.\n",
    "A perfect model has:\n",
    "- TPR = 1 (detect all positives)\n",
    "- FPR = 0 (no false alarms)\n",
    "\n",
    "The ideal ROC curve is a steep rise towards the top-left corner.\n",
    "\n",
    "#### zero_division=0\n",
    "\n",
    "- only for f1, precision and recall because they involve division where the denominator can be zero: some labels might never       be predicted (y_pred = 0 for all samples), or they might not appear in the true_labels (y_true = 0 for all samples)\n",
    "- ROC AUC: works with probabilities and does not involve division by zero\n",
    "- Precision-Recall AUC: also based on ranking, so no zero division issue\n",
    "- Accuracy: just compares exact matches, so no zero division issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-smu3fYrIybT"
   },
   "outputs": [],
   "source": [
    "def multi_label_metrics(logits, true_labels, threshold):\n",
    "    \"\"\"\n",
    "    Compute multi-label classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - logits     : raw, unnormalized scores from the model  (numpy ndarray of shape (batch_size, num_labels))\n",
    "    - true_labels: actual labels                            (numpy ndarray of shape (batch_size, num_labels))\n",
    "    - threshold  : decision threshold for converting probabilities to binary predictions\n",
    "\n",
    "    Returns:\n",
    "    - metrics: dictionary of scores\n",
    "    \"\"\"\n",
    "    #print(\">>>>>>>>>>multi_label_metrics called!<<<<<<<<<<\", flush=True)\n",
    "    print(f\"threshold: {type(threshold)} {threshold}\")\n",
    "    #print(f\"ZZZlogits: {type(logits)} {logits.shape}\\n{logits}\")                      # <class 'numpy.ndarray'> (12, 6)\n",
    "    #print(f\"ZZZtrue_labels: {type(true_labels)} {true_labels.shape}\\n{true_labels}\")  # <class 'numpy.ndarray'> (12, 6)\n",
    "\n",
    "    # Ensure logits is a PyTorch tensor before applying sigmoid\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.as_tensor(logits)\n",
    "    #print(f\"ZZZlogits: {type(logits)} {logits.shape}\\n{logits}\")                      # <class 'torch.Tensor'> torch.Size([12, 6])\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.sigmoid(logits).detach().cpu().numpy()  # Convert to NumPy safely:\n",
    "                                                          # - detach() remove the tensor from the computation graph,\n",
    "                                                          #   making it a regular tensor without gradients\n",
    "                                                          # - cpu() moves the tensor from the GPU to the CPU before converting to NumPy\n",
    "    #print(f\"ZZZprobs: {type(probs)} {probs.shape}\\n{probs}\")  # <class 'numpy.ndarray'> (12, 6)\n",
    "\n",
    "    # Apply threshold to get binary predictions\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    #print(f\"ZZZpreds: {type(preds)} {preds.shape}\\n{preds}\")  # <class 'numpy.ndarray'> (12, 6)\n",
    "\n",
    "    # Compute metrics\n",
    "    f1                   = f1_score               (y_true=true_labels, y_pred=preds, average=training_average, zero_division=0)\n",
    "    precision            = precision_score        (y_true=true_labels, y_pred=preds, average=training_average, zero_division=0)\n",
    "    recall               = recall_score           (y_true=true_labels, y_pred=preds, average=training_average, zero_division=0)\n",
    "\n",
    "    # Identify valid labels (those with both 0s and 1s in 'y_true')\n",
    "    valid_labels = np.where((true_labels.sum(axis=0) > 0) & (true_labels.sum(axis=0) < true_labels.shape[0]))[0]\n",
    "\n",
    "    if len(valid_labels) > 0:\n",
    "        roc_auc              = np.mean([roc_auc_score          (y_true=true_labels[:, i], y_score=probs[:, i]) for i in valid_labels])\n",
    "        precision_recall_auc = np.mean([average_precision_score(y_true=true_labels[:, i], y_score=probs[:, i]) for i in valid_labels])\n",
    "    else:\n",
    "        roc_auc              = np.nan  # Set to NaN if no valid labels exist\n",
    "        precision_recall_auc = np.nan  # Set to NaN if no valid labels exist\n",
    "\n",
    "    subset_acc = accuracy_score(true_labels, preds)  # Subset accuracy (requires exact match per sample)\n",
    "    hamming    = hamming_loss(true_labels, preds)    # Better for imbalanced multi-label tasks\n",
    "\n",
    "    metrics = {\n",
    "        'f1'                  : f1,\n",
    "        'precision'           : precision,\n",
    "        'recall'              : recall,\n",
    "        'roc_auc'             : roc_auc,               # Avoid warning by checking valid labels\n",
    "        'precision_recall_auc': precision_recall_auc,  # Avoid warning by checking valid labels\n",
    "        'subset_accuracy'     : subset_acc,\n",
    "        'hamming_loss'        : hamming\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIORbGwIDH-x"
   },
   "outputs": [],
   "source": [
    "# Evaluation batch per batch\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    #print(f\"p.predictions: {type(p.predictions)} {p.predictions.shape}\\n{p.predictions[:5]}\")\n",
    "    #print(f\"p.label_ids: {type(p.label_ids)} {p.label_ids.shape}\\n{p.label_ids[:5]}\")\n",
    "    print(f\"threshold: {type(threshold)} {threshold}\")\n",
    "\n",
    "    preds  = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(preds,p.label_ids, threshold)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2elPEoowcJ6"
   },
   "source": [
    "### HF transformer Trainer and CustomTrainer\n",
    "Abstracts the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR2GmpvDqbuZ"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir                  = './training_results',  # where model predictions and checkpoints will be written during training\n",
    "    overwrite_output_dir        = True,\n",
    "    save_steps                  = 500,\n",
    "    save_total_limit            = 2,\n",
    "    eval_strategy               = 'epoch',               # Evaluate at the end of each epoch\n",
    "    save_strategy               = 'epoch',               # Save checkpoints every epoch\n",
    "    learning_rate               = learning_rate,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size  = batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    num_train_epochs            = epochs,\n",
    "    weight_decay                = 0.01,\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = metric_name,\n",
    "    fp16                        = fp,\n",
    "    run_name                    = run_name,\n",
    "    report_to                   = 'none'                 # Disable wandb if not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Isb6aMV2bWJ8"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, model, *args, loss_fn=None, **kwargs):\n",
    "        super().__init__(model, *args, **kwargs)\n",
    "        self.loss_fn = loss_fn\n",
    "        #print(f\">>>>>>>>>>CustomTrainer initialized with loss_fn: {loss_fn}<<<<<<<<<<\")\n",
    "\n",
    "    \"\"\"\n",
    "    # No print in compute_loss because out of memory because prints are batch per batch\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "    #print(f\"inputs passed to compute_loss: {inputs.keys()}\")\n",
    "    #input_ids             = inputs['input_ids']                        # shape: batch_size, sequence_length\n",
    "    #attention_mask        = inputs['attention_mask']                   # shape: batch_size, sequence_length\n",
    "    #global_attention_mask = inputs.get('global_attention_mask', None)  # shape: batch_size, sequence_length; optional as LongFormer specific\n",
    "    labels                = inputs.pop('labels', None)                 # shape: batch_size, num_labels; needed for loss computation, not required by the model\n",
    "\n",
    "    #outputs = model(**inputs, global_attention_mask=global_attention_mask)  # Forward pass\n",
    "    # Forward pass\n",
    "    #outputs = model(\n",
    "    #    input_ids             = input_ids,\n",
    "    #    attention_mask        = attention_mask,\n",
    "    #    global_attention_mask = global_attention_mask,\n",
    "    #    labels                = labels\n",
    "    #)\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    #print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")\n",
    "    logits = outputs.logits  # shape: (batch_size, num_labels)\n",
    "\n",
    "    # If labels are provided, compute loss\n",
    "    if labels is not None:\n",
    "      # Use the custom loss function if provided\n",
    "      if self.loss_fn is not None:\n",
    "        loss = self.loss_fn(logits, labels)  # Compute weighted loss\n",
    "      else:\n",
    "        # Default loss: BCEWithLogitsLoss\n",
    "        loss_fn = BCEWithLogitsLoss()\n",
    "        loss    = loss_fn(logits, labels)    # Compute loss\n",
    "      return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    # If no labels, return outputs only, for evaluation or prediction\n",
    "    return outputs\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        #print(f\">>>>>>>>>>compute_loss called!<<<<<<<<<<\", flush=True)\n",
    "        labels  = inputs.get('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits  # (batch_size, num_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device).float()  # Ensure same device\n",
    "\n",
    "            if self.loss_fn is not None:\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                logging.info(f\"Step Loss ({self.loss_fn.__class__.__name__}): {loss.item():.6f}\")  # Log loss value\n",
    "                #print(f\"Epoch {self.state.epoch:.0f}, Step {self.state.global_step}: Loss ({self.loss_fn.__class__.__name__}): {loss.item():.6f}\", flush=True)\n",
    "            else:\n",
    "                loss_fn = BCEWithLogitsLoss()\n",
    "                loss    = loss_fn(logits, labels)\n",
    "                logging.info(f\"Step Loss (BCEWithLogitsLoss): {loss.item():.6f}\")                  # Log loss value\n",
    "                #print(f\"Epoch {self.state.epoch:.0f}, Step {self.state.global_step}: Loss (BCEWithLogitsLoss): {loss.item():.6f}\", flush=True)\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chq_3nUz73ib",
    "outputId": "aa9c2803-90f7-4545-d46f-87693de80c48"
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = encoded_dataset[\"train\"],\n",
    "    eval_dataset    = encoded_dataset[\"validation\"],\n",
    "    compute_metrics = compute_metrics,\n",
    "    loss_fn         = focal_loss_fn,  # bce_loss_fn or focal_loss_fn\n",
    "    #callbacks       = [LossLoggerCallback(), MetricsLoggerCallback(), ProgressLoggerCallback()]  # Attach logging callbacks\n",
    ")\n",
    "\n",
    "#trainer = Trainer(\n",
    "#    model           = model,\n",
    "#    args            = training_args,\n",
    "#    train_dataset   = encoded_dataset[\"train\"],\n",
    "#    eval_dataset    = encoded_dataset[\"validation\"],\n",
    "#    compute_metrics = compute_metrics,\n",
    "#)\n",
    "\n",
    "print(f\"trainer: {type(trainer)} {trainer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3kPFNOG-3vL"
   },
   "source": [
    "### trainer.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "V_k5uxpKwcJ6",
    "outputId": "57b2241c-ea6b-46d1-cc53-5ad15110e2ac"
   },
   "outputs": [],
   "source": [
    "trainer_train = trainer.train()\n",
    "\n",
    "print(f\"trainer_train: {type(trainer_train)} len={len(trainer_train)}\\n{trainer_train}\")\n",
    "print()\n",
    "print(f\"trainer_train.metrics: {type(trainer_train.metrics)} len={len(trainer_train.metrics)}\\n{json.dumps(trainer_train.metrics, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2SMR-lI8Y8r",
    "outputId": "5f4710c5-13bd-40f5-e351-91b864c2b902"
   },
   "outputs": [],
   "source": [
    "print(\"trainer.train successfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uai-d2z2IMaZ"
   },
   "source": [
    "### trainer.train: save locally and upload to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvhT1c_h_oul",
    "outputId": "2865af1b-3f11-4582-f2f4-03b8e448b988"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "    name               = \"trainer_train\"\n",
    "    trainer_train_path = f\"{name}.json\"\n",
    "\n",
    "    with open(trainer_train_path, \"w\") as f:\n",
    "        json.dump(trainer_train, f)\n",
    "\n",
    "    print(f\"{name} results successfully saved locally to {trainer_train_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = trainer_train_path,\n",
    "        path_in_repo    = trainer_train_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} results successfully uploaded to HF Hub as {trainer_train_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as2omZpPJNO6"
   },
   "source": [
    "### trainer.train: check that the uploaded file can be downloaded\n",
    "File locally downloaded to:\n",
    "/root/.cache/huggingface/hub/datasets-claudelepere-skill_classification/snapshots/full_commit_hash/trainer_train_results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "105c8f01435d41599d3ebc175b709921",
      "1d407254aed64cd18b42bb2ee389668f",
      "bb60d37d03344947a8cfe5b6dfdb0782",
      "2aa22974016b42158891e7898c95673e",
      "037b9d0ea4d444818d9be03cc11f4ddf",
      "9b274fc615b44e9586381570cccbc8df",
      "ce778357366c4f5ba360c404d5aa103c",
      "1de526a1e6f34f56a30afa3d2bce27b5",
      "d1e911138e954501841af5fb28bf9ec3",
      "093c08d042444f48ac57a1f582efa158",
      "ad3772f1e04d40c08803d960f05ef795"
     ]
    },
    "id": "PJ2D28X2-GfV",
    "outputId": "ccbe6bf8-35e3-4a5f-9839-df38d6358c15"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "  file_path = hf_hub_download(repo_type=\"dataset\", repo_id=repo_id, filename=trainer_train_path)\n",
    "\n",
    "  print(f\"file_path: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj0Kz_Wjo5mr"
   },
   "source": [
    "##Upload tokenizer and model to HF Hub and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876,
     "referenced_widgets": [
      "d24048b96e59405d822b643d2b0b1f08",
      "c9da149276324fadaee0cc00654fb4df",
      "a264abe78f174c2786181eac60d848a2",
      "9ba4d88f1c2240cc8bba58bb5802f66a",
      "08a1c3760a7d41a0bbde9a54710a7f42",
      "8fe8ee2bd13343f2ac0c5d99dc29f1f6",
      "af0bc0093d4641368311b5147cefb18b",
      "74d70388214843cda8586441f3b3f01c",
      "8d0c30f1f5c7414ebd8deb5494bde1c0",
      "93e2b241a87945dbb9fc8668b13a383b",
      "a86566914ecd4dc3840c3dfba4be966a",
      "df47e2d0c6254a93822d7d104873f025",
      "502df51d7fc448f6858f85f8849e3c8c",
      "357ee83ba2e4410e8f4093bcb3805763",
      "5c1f7f8ac3904021a7e6d5b7f34f8d2c",
      "7b59d3522bfc46adb13e2b441ecec328",
      "c92f60eca8994fe1a41512a2e831a6a6",
      "496d3f355e0941b19521b340b8d753f3",
      "7989c14eb21d40568d1beda31779d251",
      "30c731001ec444418a6f88ef8e37e2d2",
      "42b69991d78b4c9d9da50c4f73f59512",
      "aa8aecdefeff4b67b324cbc7b618fcec",
      "23ea60a728ed45049be6dd205179ef1d",
      "953260863f2f4b76a20dab9e1f686d92",
      "38e7d6d3d5b34e7baaa027768b6b7086",
      "9891b3af14ac429883b0b016f1b665e7",
      "1a4b7f38186948e78ebe18f3a4184703",
      "a547896d783941cfbcc67f58f20e941a",
      "21082a9c197d4a2d983ded80de3bc8be",
      "29204b2c94b245ccb8f649b308eeb928",
      "ff3a4920a73b47909ce3c56656c66b95",
      "7e277c6987f143fdbf453d48af403212",
      "5f92f6dc64b44ec3b3f2cf2b96c200d0",
      "cd8f2555b1a943a699701890d1dd8594",
      "51b8ff6a01bb4f6cafed6bb9756c6fff",
      "2ddb1f7c45f04eacb9917f4d9c21d591",
      "9de55d7b03104bf9970d92c78be6e855",
      "8c8305f7a6604107a3e0bc0bab6b89f4",
      "113d547d95aa41499de6b073cff11aee",
      "0cf64ba6152149ed9e24a823bbff43f2",
      "7578c6c0aa8148c9a97c374d3e87bfdd",
      "4decce13326a4fb38cc63220eec125e4",
      "e7dbbbb517a7456caa4d3ed8484a1c72",
      "74a8779161184c2da4f1c159798a2ebc",
      "7d54e7dcbfcd4793bcab50f03df51907",
      "19270961b8294af6a87332b8cc42c039",
      "cc7b28a1e6ce4932b1321179db853321",
      "4657910f387942c39110acfd49acb419",
      "18aa7304204049ef9c1aed9e75888f3d",
      "7369a286d77e41f8bfd5a7a9025a2c01",
      "5540027489934cbfb048fe29e416561d",
      "a0c8e1a75d4d41a7aad89f936e793327",
      "817aa66011304d368c3330bc2160ec70",
      "af634893bd1d4846a48c91bb50e8b708",
      "c94f2d5920444503b28cca6e4d1ad691",
      "63f460ed8b7445fbb838610f1b92eb27",
      "6f1f9db5e50a44ad93acc70032b8857c",
      "64f0f2356aeb40dbb51a94a0b66a753b",
      "4ab199ddb4fb4eb8877d35104d977af6",
      "f02c34d4c28a4597938604766f4777c7",
      "a552d1221fe344fa897962e3182872ef",
      "f19278f8c98a4c21a00c6be7db847e77",
      "11a750f8556f48c8bff521b636a60cc6",
      "43eaeda7067c42708a0d9f362941e681",
      "6daa5d7fa925473fb4f2b9af6d994f25",
      "3d9d74e8839f4c90b1fceac1a6d236dc",
      "96f27f1101b748a7b208da1c709b0623",
      "df021f04557e4b5b85e47784b4099ecb",
      "3621f4cb83b048ea9dda8436c3efcb70",
      "a4e9285a4cd14650b4d874de12e63b62",
      "a0b5660a0ec8499c898a1d904a17e785",
      "6b92f303f4604ec48dc3d9da378626de",
      "c8cf70910ec64f60aaa62febad5ba5d3",
      "ba3c255302f14277a45b058e6f5c863d",
      "7f767194a0a84086b5398eca23ab1f32",
      "7b7d55d21aad4c5c8b4f7b4d7c8118a5",
      "7098c55b3e8e44e98cc711aff547fab6",
      "0e619e76b149420480f3da298ff6ba3a",
      "53093c3cc7af48a2bf09200c4a370819",
      "b0b77abf0a2f4946b4dc45bbb8754f96",
      "6d4149448c0a48a18c7628ff03fbc495",
      "4a74cce4853b4eccbe71d0d483d37a7c",
      "ff9a7ffc6dae4216843dd8667de27ab9",
      "15d8f58686b24ac58d80fe917ee6e3dd",
      "032270e6e3134f93a4dd4907d0411529",
      "bc2fa7a417514e90be5a9a4ea89bad14",
      "cba4ecef93214917acd1ed0f31bbd776",
      "998d39eed5c6494a8961a40bf23be0a4",
      "5462b13ddf884d7b97f9892cf456080d",
      "eb2e30bd337f4f32adbeab8348a35bed",
      "c9d67fa1bbf045d5b22107ef89c7883b",
      "0921454258b74cdaa124c89dc360eae5",
      "7ba3e2b3f87d4a888befe8c9b580c026",
      "82378a1bd1cf4f249c7af258d3d71dc5",
      "8f4bff1dd1934af7af8ffb4bd8ed5247",
      "d921c9a2ab7941329541a6c8542bcf36",
      "044c91293b1943fda0477063e5061cb8",
      "250cf6306d6941b6b2c3106b57393e0c",
      "f65ebef7a8394eff9e05c5d349d4aa84",
      "bc5881023acb4707aed1297712e16269",
      "e66cd612d2764aaca822977f8cafa5be",
      "cba9b51aff5f42c0a370d3c6dd452edf",
      "5474901a861f4e64a75a653f9c3a8520",
      "f4bae680737142c6846e00a059b67b2d",
      "8daf635acb5d45b5bec1bac659604593",
      "e823347f62f84d4fa4959e9af20f5a03",
      "78b6694c00ae4ce7927a566579f66f14",
      "936cd91a373044b180e47d923d2983b6",
      "7bddbef2172f4419b3af886e0d499d5e",
      "1df8cfda1b454509bbd62ce814018d97"
     ]
    },
    "id": "uelp4Rg0pmwq",
    "outputId": "8a7946e1-9be8-4ca6-900b-e075147d20f0"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "    # Upload\n",
    "    commit_message = f\"tokenizer_{timestamp}\"\n",
    "    tokenizer.push_to_hub(repo_id, commit_message=commit_message)  # commit_message as named parameter\n",
    "\n",
    "    commit_message = f\"model_{timestamp}\"\n",
    "    model.push_to_hub(    repo_id, commit_message=commit_message)  # commit_message as named parameter\n",
    "\n",
    "    print(f\"tokenizer and model successfully uploaded to HF Hub at {repo_id}\")\n",
    "\n",
    "    # Check\n",
    "    def check_upload(repo_id):\n",
    "        print()\n",
    "        print(\"Tokenizer\")\n",
    "        tokenizer = LongformerTokenizerFast.from_pretrained(repo_id)\n",
    "        print()\n",
    "        print(\"Model\")\n",
    "        model = LongformerForSequenceClassification.from_pretrained(repo_id)\n",
    "        print()\n",
    "\n",
    "        inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")\n",
    "\n",
    "    # To check if the upload was successful, download the tokenizer and the model\n",
    "    check_upload(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVozoTnW1Mt9"
   },
   "source": [
    "## Evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05FkezueB6_J"
   },
   "source": [
    "### Evaluation 1: trainer.evaluate\n",
    "trainer.evaluate uses a fixed threshold of 0.5 to convert logits into binary labels, which is often suboptimal for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "HMKEjsRk5ah6",
    "outputId": "3236714a-c136-49e4-ba1e-e6a94d6d16e4"
   },
   "outputs": [],
   "source": [
    "evaluation_trainer_evaluate_metrics = trainer.evaluate(\n",
    "    #eval_dataset = encoded_dataset[\"validation\"],  # by default, trainer.evaluate() evaluates the dataset passed as eval_dataset during training\n",
    "    metric_key_prefix=\"eval\"                       # prefix for the evaluation metrics\n",
    ")\n",
    "\n",
    "print(f\"evaluation_trainer_evaluate_metrics: {type(evaluation_trainer_evaluate_metrics)} len={len(evaluation_trainer_evaluate_metrics)}\\n{json.dumps(evaluation_trainer_evaluate_metrics, indent=4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_O-_tVHg-lUe",
    "outputId": "fafc4e7c-ba7a-48dc-ef3f-27eb81ab4e41"
   },
   "outputs": [],
   "source": [
    "print(\"evaluation 1: trainer.evaluate: successfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w5BmumZ6tDS"
   },
   "source": [
    "### Evaluation 1: trainer.evaluate: save locally and upload to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMlebJ83LRYG",
    "outputId": "27a54ee9-dda2-4a0a-ae17-56d2bd2781ec"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "    name                             = \"evaluation_trainer_evaluate\"\n",
    "    evaluation_trainer_evaluate_path = f\"{name}.json\"\n",
    "\n",
    "    with open(evaluation_trainer_evaluate_path, \"w\") as f:\n",
    "        json.dump(evaluation_trainer_evaluate_metrics, f)\n",
    "\n",
    "    print(f\"{name} successfully saved locally to {evaluation_trainer_evaluate_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = evaluation_trainer_evaluate_path,\n",
    "        path_in_repo    = evaluation_trainer_evaluate_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} successfully uploaded to HF Hub as {evaluation_trainer_evaluate_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxYFDrB57nM3"
   },
   "source": [
    "### Evaluation 2: trainer.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvBaTA62X7xV"
   },
   "outputs": [],
   "source": [
    "def predict_with_optimized_thresholds(trainer, dataset, id2label, threshold_tuning=False, thresholds=None):\n",
    "    \"\"\"\n",
    "    Predicts using trainer.predict(), with optional threshold tuning, using NumPy arrays, and not PyTorch tensors\n",
    "\n",
    "    Parameters:\n",
    "    - trainer                         : Hugging Face Trainer or CustomTrainer instance\n",
    "    - dataset                         : Dataset to predict on\n",
    "    - id2label                        : Dictionary mapping label indices (int) to label names (string)\n",
    "    - threshold_tuning                : Boolean to enable threshold tuning per class (aka per label)\n",
    "    - thresholds       (numpy.ndarray): Custom thresholds for classification\n",
    "\n",
    "    Returns:\n",
    "    if threshold_tuning:\n",
    "        - best_thresholds      (numpy.ndarray): optimized threshold per class (aka per label)\n",
    "        - best_thresholds_dict (dict)         : optimized threshold per class (aka per label)\n",
    "        - best_metrics         (dict)         : best F1, best precision, best recall per class (aka per label)\n",
    "        - best_preds           (numpy.ndarray): best predictions per class (aka per label)\n",
    "    else:\n",
    "        - thresholds      (numpy.ndarray): fixed threshold per class (aka per label)\n",
    "        - thresholds_dict (dict)         : fixed threshold per class (aka per label)\n",
    "        - metrics         (dict)         : computed with provided thresholds or default to 0.5\n",
    "        - preds           (numpy.ndarray): predictions with provided thresholds or default to 0.5\n",
    "    \"\"\"\n",
    "    # Predict\n",
    "    predictions_output = trainer.predict(dataset)\n",
    "    logits             = predictions_output.predictions\n",
    "    true_labels        = predictions_output.label_ids\n",
    "\n",
    "    # Convert logits to probabilities (with np, not with torch)\n",
    "    probs = 1 / (1 + np.exp(-logits))  # Sigmoid function\n",
    "\n",
    "    num_labels           = len(id2label)\n",
    "    best_thresholds      = None\n",
    "    best_thresholds_dict = None\n",
    "    best_metrics         = None\n",
    "    best_preds           = None\n",
    "\n",
    "    if threshold_tuning:\n",
    "        threshold_candidates = np.linspace(0.05, 0.95, 19)\n",
    "        best_thresholds      = np.zeros(num_labels)\n",
    "        best_metrics         = {label: {'f1': 0.0, 'precision': 0.0, 'recall': 0.0} for label in id2label.values()}\n",
    "\n",
    "        # Iterate over each label to find the best threshold\n",
    "        for label_idx, label in id2label.items():\n",
    "            # Predictions for the current label across all threshold candidates\n",
    "            preds = probs[:, label_idx][:, None] > threshold_candidates  # Create a matrix of shape (num_samples, num_thresholds)\n",
    "\n",
    "            # Compute precision, recall, F1 for all thresholds at once for the current label\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                np.tile(true_labels[:, label_idx], (len(threshold_candidates), 1)).T, preds, average=None, zero_division=0\n",
    "            )\n",
    "\n",
    "            # Find the best threshold based on F1 for the current label\n",
    "            best_idx                   = np.argmax(f1)\n",
    "            best_thresholds[label_idx] = threshold_candidates[best_idx]\n",
    "            best_metrics[label]        = {'f1': f1[best_idx], 'precision': precision[best_idx], 'recall': recall[best_idx]}\n",
    "\n",
    "        best_thresholds_dict = {id2label[i]: best_thresholds[i].item() for i in range(len(best_thresholds))}\n",
    "\n",
    "        # Generate predictions using the optimized threshold for each label\n",
    "        best_preds = np.zeros_like(true_labels, dtype=int)\n",
    "        for label_idx, label in id2label.items():\n",
    "            best_preds[:, label_idx] = (probs[:, label_idx] > best_thresholds[label_idx]).astype(int)\n",
    "\n",
    "        #print(\"==== best_thresholds, best_threshold_dict and best_metrics ====\")\n",
    "        #print(f\"best_thresholds:      {type(best_thresholds)} shape={best_thresholds.shape}\\n{best_thresholds}\")                # <class 'numpy.ndarray'> shape=(6,)\n",
    "        #print(f\"best_thresholds_dict: {type(best_thresholds_dict)} len={len(best_thresholds_dict)}\\n{best_thresholds_dict}\")    # <class 'dict'> len=6\n",
    "        #print(f\"best_metrics:         {type(best_metrics)} len={len(best_metrics)}\\n{json.dumps(best_metrics, indent=4)}\")      # <class 'dict'> len=6\n",
    "        #print(\"===============================================================\")\n",
    "        #print()\n",
    "\n",
    "    # ==== If not threshold_tuning ====\n",
    "\n",
    "    # Apply provided thresholds or default to 0.5\n",
    "    thresholds_fixed = thresholds if thresholds is not None else np.full(num_labels, 0.5)\n",
    "\n",
    "    # Compute predictions with fixed thresholds\n",
    "    preds_fixed = (probs > thresholds_fixed).astype(int)\n",
    "\n",
    "    # Compute metrics in one step (no loop)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds_fixed, average=None, zero_division=0)\n",
    "\n",
    "    # Convert to dict format\n",
    "    metrics_fixed         = {id2label[i]: {'f1': f1[i], 'precision': precision[i], 'recall': recall[i]} for i in range(num_labels)}\n",
    "    thresholds_fixed_dict = {id2label[i]: thresholds_fixed[i].item() for i in range(num_labels)}\n",
    "\n",
    "    #print(\"==== provided thresholds and metrics ====\")\n",
    "    #print(f\"thresholds_fixed     : {type(thresholds_fixed)} shape={thresholds_fixed.shape}\\n{thresholds_fixed}\")              # <class 'numpy.ndarray'> shape=(6,)\n",
    "    #print(f\"thresholds_fixed_dict: {type(thresholds_fixed_dict)} len={len(thresholds_fixed_dict)}\\n{thresholds_fixed_dict}\")  # <class 'dict'> len=6\n",
    "    #print(f\"metrics_fixed        : {type(metrics_fixed)} len={len(metrics_fixed)}\\n{json.dumps(metrics_fixed, indent=4)}\")    # <class 'dict'> len=6\n",
    "    #print(\"===============================================================\")\n",
    "    #print()\n",
    "\n",
    "    thresholds      = best_thresholds      if threshold_tuning else thresholds_fixed\n",
    "    thresholds_dict = best_thresholds_dict if threshold_tuning else thresholds_fixed_dict\n",
    "    metrics         = best_metrics         if threshold_tuning else metrics_fixed\n",
    "    preds           = best_preds           if threshold_tuning else preds_fixed\n",
    "\n",
    "    # Compute micro average\n",
    "    #   compute metrics globally by summing all TP, FP, FN across all labels\n",
    "    #   good for overall performance assessment\n",
    "    #   dominated by frequent labels: if most samples belong to a few labels, if favors them\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, preds, average='micro', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Compute macro average\n",
    "    #   each label is treated equally, regardless of how often it appears\n",
    "    #   good for evaluating rare labels\n",
    "    #   sensitive to rare labels: if rare labels perform poorly, macro F1 will drop\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Compute weighted average\n",
    "    #   like macro, but weights each label's F1 based on its frequency\n",
    "    #   balances between micro and macro by considering both label importance and prevalence\n",
    "    #   useful if class imbalance exists but you still want per-label influence\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    global_metrics = {\n",
    "        'micro':    {'f1': micro_f1,    'precision': micro_precision,    'recall': micro_recall},\n",
    "        'macro':    {'f1': macro_f1,    'precision': macro_precision,    'recall': macro_recall},\n",
    "        'weighted': {'f1': weighted_f1, 'precision': weighted_precision, 'recall': weighted_recall}\n",
    "    }\n",
    "\n",
    "    return thresholds, thresholds_dict, metrics, global_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YZ2WcwI7IBl"
   },
   "source": [
    "### Evaluation 2: calculate metrics and optimized thresholds\n",
    "\n",
    "- First, to **calculate** the optimized thresholds, threshold_tuning = True and thresholds = None.\n",
    "- After, to **use** these optimized thresholds, threshold_tuning = False and thresholds = the optimized thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TphO3ifmBLDf",
    "outputId": "015ed2a4-bcfb-40eb-88ae-f67cf04dcfbd"
   },
   "outputs": [],
   "source": [
    "# with best_thresholds (threshold_tuning = True and thresholds = None)\n",
    "optimized_thresholds, optimized_thresholds_dict, evaluation_trainer_predict_metrics, evaluation_trainer_predict_global_metrics = predict_with_optimized_thresholds(\n",
    "    trainer, validation_dataset, id2label, threshold_tuning=True, thresholds=None)\n",
    "\n",
    "print(\"==== with best thresholds ====\")\n",
    "print(f\"optimized_thresholds                     : {type(optimized_thresholds)} shape={optimized_thresholds.shape} {optimized_thresholds}\")\n",
    "print(f\"optimized_thresholds_dict                : {type(optimized_thresholds_dict)} len={len(optimized_thresholds_dict)}\\n{optimized_thresholds_dict}\")\n",
    "print(f\"evaluation_trainer_predict_metrics       : {type(evaluation_trainer_predict_metrics)} len={len(evaluation_trainer_predict_metrics)}\\n{json.dumps(evaluation_trainer_predict_metrics, indent=4)}\")\n",
    "print(f\"evaluation_trainer_predict_global_metrics: {type(evaluation_trainer_predict_global_metrics)} len={len(evaluation_trainer_predict_global_metrics)}\\n{json.dumps(evaluation_trainer_predict_global_metrics, indent=4)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# with thresholds_fixed=0.5 (threshold_tuning = False and thresholds = None)\n",
    "thresholds, thresholds_dict, evaluation_trainer_predict_metrics_thr05, evaluation_trainer_predict_global_metrics_thr05 = predict_with_optimized_thresholds(\n",
    "    trainer, validation_dataset, id2label, threshold_tuning=False, thresholds=None)\n",
    "\n",
    "print(\"==== with default fixed thresholds = 0.5 ====\")\n",
    "print(f\"thresholds                                     : {type(thresholds)} shape={thresholds.shape} {thresholds}\")\n",
    "print(f\"thresholds_dict                                : {type(thresholds_dict)} len={len(thresholds_dict)}\\n{thresholds_dict}\")\n",
    "print(f\"evaluation_trainer_predict_metrics_thr05       : {type(evaluation_trainer_predict_metrics_thr05)} len={len(evaluation_trainer_predict_metrics_thr05)}\\n{json.dumps(evaluation_trainer_predict_metrics_thr05, indent=4)}\")\n",
    "print(f\"evaluation_trainer_predict_global_metrics_thr05: {type(evaluation_trainer_predict_global_metrics_thr05)} len={len(evaluation_trainer_predict_global_metrics_thr05)}\\n{json.dumps(evaluation_trainer_predict_global_metrics_thr05, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gc9XZPkpDFdR",
    "outputId": "c69dd1c7-6fc8-4d7e-f898-fa1f6dc5cea4"
   },
   "outputs": [],
   "source": [
    "print(\"evaluation 2: trainer.predict: successfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwV60_UQ-HXd"
   },
   "source": [
    "### Evaluation 2: trainer.predict: save locally and upload to HF Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14zP2d72-WXk",
    "outputId": "4a69884f-6049-4248-a008-009389e31ecc"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "    name                            = \"evaluation_trainer_predict\"\n",
    "    evaluation_trainer_predict_path = f\"{name}.json\"\n",
    "\n",
    "    with open(evaluation_trainer_predict_path, \"w\") as f:\n",
    "        json.dump(evaluation_trainer_predict_metrics, f)\n",
    "\n",
    "    print(f\"{name} successfully saved locally to {evaluation_trainer_predict_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = evaluation_trainer_predict_path,\n",
    "        path_in_repo    = evaluation_trainer_predict_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} successfully uploaded to HF Hub as {evaluation_trainer_predict_path}\")\n",
    "\n",
    "    name_thr05                            = \"evaluation_trainer_predict_thr05\"\n",
    "    evaluation_trainer_predict_path_thr05 = f\"{name_thr05}.json\"\n",
    "\n",
    "    with open(evaluation_trainer_predict_path_thr05, \"w\") as f:\n",
    "        json.dump(evaluation_trainer_predict_metrics_thr05, f)\n",
    "\n",
    "    print(f\"{name_thr05} successfully saved locally to {evaluation_trainer_predict_path_thr05}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = evaluation_trainer_predict_path_thr05,\n",
    "        path_in_repo    = evaluation_trainer_predict_path_thr05,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name_thr05}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name_thr05} successfully uploaded to HF Hub as {evaluation_trainer_predict_path_thr05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f127duCt-Nhg"
   },
   "source": [
    "### Evaluation 2: optimized thresholds: save locally (as a dict) and upload to HF Hub (as a JSON file in repo 'model')\n",
    "optimized_thresholds: <class 'numpy.ndarray'> shape=(6,) but JSON doesn't support NumPy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZjQfyNP-J46",
    "outputId": "484dfb33-1de3-43eb-893f-67100b61b12a"
   },
   "outputs": [],
   "source": [
    "if threshold_tuning is True and upload_to_HF is True:\n",
    "    name                      = \"optimized_thresholds\"\n",
    "    optimized_thresholds_path = f\"{name}.json\"\n",
    "\n",
    "    with open(optimized_thresholds_path, \"w\") as f:\n",
    "        json.dump(optimized_thresholds_dict, f, indent=4)\n",
    "\n",
    "    print(f\"{optimized_thresholds_dict} successfully saved locally to {optimized_thresholds_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = optimized_thresholds_path,\n",
    "        path_in_repo    = optimized_thresholds_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'model',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "        )\n",
    "\n",
    "    print(f\"{name} successfully uploaded to HF Hub as {optimized_thresholds_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noWOBOOd9hP7"
   },
   "source": [
    "### Evaluation 3: model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XGQ4QNv9LsL"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_with_threshold(model, dataset, optimized_thresholds, id2label, batch_size=8):\n",
    "    \"\"\"\n",
    "    Compute metrics during evaluation or test, by applying tuned thresholds\n",
    "\n",
    "    Parameters:\n",
    "    - model                                               : Hugging Face model\n",
    "    - dataset                                             : Dataset to predict on\n",
    "    - optimized_thresholds (list or NumPy array of floats): Optimized thresholds for each label\n",
    "    - id2label                                            : Dictionary mapping label indices (int) to label names (string)\n",
    "    - batch_size                                          : Batch size for prediction. Defaults to 8\n",
    "    Returns:\n",
    "    - metrics (dict)\n",
    "\n",
    "    Compute metrics during evaluation or test, by applying optimized thresholds\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)    # Move model to GPU/CPU\n",
    "    model.eval()        # Set model to evaluation mode\n",
    "\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            labels  = batch.pop('labels')                           # Keep labels on CPU\n",
    "            inputs  = {k: v.to(device) for k, v in batch.items()}   # Move inputs to device\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            all_logits.append(outputs.logits.cpu())     # Keep logits as tensors, move to CPU\n",
    "            all_labels.append(labels)                   # Labels remain on CPU\n",
    "\n",
    "    # Stack tensors\n",
    "    logits = torch.cat(all_logits, dim=0).to(device)   # shape = (num_samples, num_labels), move to device\n",
    "    labels = torch.cat(all_labels, dim=0).to(device)   # shape = (num_samples, num_labels), move to device\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.sigmoid(logits)           # shape = (num_samples, num_labels)\n",
    "\n",
    "    # Apply per-class tuned thresholds (element-wise comparison)\n",
    "    thresholds = torch.tensor(optimized_thresholds, dtype=torch.float32, device=device)  # Convert tuned_thresholds to tensor\n",
    "    preds      = (probs > thresholds).int()                                              # Convert to binary predictions (1 or 0)\n",
    "\n",
    "    # Compute TP, FP, FN, TN\n",
    "    TP = ((preds == 1) & (labels == 1)).sum(dim=0).float()\n",
    "    TN = ((preds == 0) & (labels == 0)).sum(dim=0).float()\n",
    "    FP = ((preds == 1) & (labels == 0)).sum(dim=0).float()\n",
    "    FN = ((preds == 0) & (labels == 1)).sum(dim=0).float()\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    precision_per_class = TP / (TP + FP + 1e-8)\n",
    "    recall_per_class    = TP / (TP + FN + 1e-8)\n",
    "    f1_per_class        = 2 * (precision_per_class * recall_per_class) / (precision_per_class + recall_per_class + 1e-8)\n",
    "\n",
    "    # Compute averaged metrics\n",
    "    precision = precision_per_class.mean()\n",
    "    recall    = recall_per_class.mean()\n",
    "    f1        = f1_per_class.mean()\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = (preds == labels).float().mean()\n",
    "\n",
    "    # Convert to NumPy for ROC-AUC and PR-AUC\n",
    "    labels_np = labels.cpu().numpy()  # Move to CPU before converting\n",
    "    probs_np  = probs.cpu().numpy()   # Move to CPU before converting\n",
    "\n",
    "    # Compute ROC-AUC and PR-AUC\n",
    "    roc_auc = torch.tensor(roc_auc_score(labels_np, probs_np, average=evaluation_average, multi_class='ovr'))\n",
    "    pr_auc  = torch.tensor(average_precision_score(labels_np, probs_np, average=evaluation_average))\n",
    "\n",
    "    # Convert predictions to Numpy for classification_report\n",
    "    preds_np = preds.cpu().numpy()  # Move to CPU before converting\n",
    "\n",
    "    # Generate classification report\n",
    "    class_names  = [id2label[i] for i in range(len(id2label))]\n",
    "    class_report = classification_report(labels_np, preds_np, target_names=class_names, zero_division=0)\n",
    "\n",
    "    #print(f\"\\nClassification Report:\\n{class_report}\")\n",
    "\n",
    "    # Store metrics\n",
    "    metrics = {\n",
    "        'accurary'             : accuracy.item(),\n",
    "        'precision'            : precision.item(),\n",
    "        'recall'               : recall.item(),\n",
    "        'f1'                   : f1.item(),\n",
    "        'roc_auc'              : roc_auc.item(),\n",
    "        'pr_auc'               : pr_auc.item(),\n",
    "        'per_class_precision'  : {id2label[i]: precision_per_class[i].item() for i in range(len(id2label))},\n",
    "        'per_class_recall'     : {id2label[i]: recall_per_class[i].item() for i in range(len(id2label))},\n",
    "        'per_class_f1'         : {id2label[i]: f1_per_class[i].item() for i in range(len(id2label))},\n",
    "        'classification_report': class_report,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehquE8Ii2yNT",
    "outputId": "94ae3f73-eb2b-4bc7-8b1e-b22b92cb3d90"
   },
   "outputs": [],
   "source": [
    "evaluation_model_eval_metrics = compute_metrics_with_threshold(model, validation_dataset, optimized_thresholds, id2label, batch_size=16)\n",
    "\n",
    "except_report = {k: v for k, v in evaluation_model_eval_metrics.items() if k!='classification_report'}\n",
    "report        = evaluation_model_eval_metrics['classification_report']\n",
    "print(f\"evaluation_model_eval_metrics: {type(except_report)} len={len(except_report)}\\n{json.dumps(except_report, indent=4)}\")\n",
    "print(f\"evaluation_model_eval_metrics['classification_report']: {type(report)} len={len(report)}\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lu5iSiinCUOh",
    "outputId": "8b00e711-739b-4a7b-fa49-5926174966c5"
   },
   "outputs": [],
   "source": [
    "print(\"evaluation 3: model.eval: successfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFwAT0oWHawd"
   },
   "source": [
    "### Evaluation 3: model.eval: save locally and upload to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilEmnR5jHeYs",
    "outputId": "8e1afa73-e7c1-4dc0-ce3f-cdc5d4daf949"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "    name                       = \"evaluation_model_eval\"\n",
    "    evaluation_model_eval_path = f\"{name}.json\"\n",
    "\n",
    "    with open(evaluation_model_eval_path, \"w\") as f:\n",
    "        json.dump(evaluation_model_eval_metrics, f)\n",
    "\n",
    "    print(f\"{name} successfully saved locally to {evaluation_model_eval_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = evaluation_model_eval_path,\n",
    "        path_in_repo    = evaluation_model_eval_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} successfully uploaded to HF Hub as {evaluation_model_eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYSVoKZDAq4e"
   },
   "source": [
    "## Test step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lCQusokDRda"
   },
   "source": [
    "### Test 1: trainer.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "3B6m5pYJM_0e",
    "outputId": "b3c5eb00-8bfe-4213-d63f-f4de137eb926"
   },
   "outputs": [],
   "source": [
    "test_trainer_evaluate_metrics = trainer.evaluate(\n",
    "    eval_dataset = encoded_dataset['test'],\n",
    "    metric_key_prefix='test'\n",
    ")\n",
    "\n",
    "print(f\"test_trainer_evaluate_metrics: {type(test_trainer_evaluate_metrics)} len={len(test_trainer_evaluate_metrics)}\\n{json.dumps(test_trainer_evaluate_metrics, indent=4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JuMtbi6ODIJ",
    "outputId": "b07f1b09-8f58-4744-94fd-f95de4f786d6"
   },
   "outputs": [],
   "source": [
    "print(\"test_trainer.evaluate successfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LosNwpYCOV2F"
   },
   "source": [
    "### Test 1: trainer.evaluate: save locally and upload to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuJvVmF8Oj8M",
    "outputId": "8e0ce5eb-b4a9-4491-8e5c-f251be27c5af"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "    name                       = \"test_trainer_evaluate\"\n",
    "    test_trainer_evaluate_path = f\"{name}.json\"\n",
    "\n",
    "    with open(test_trainer_evaluate_path, \"w\") as f:\n",
    "        json.dump(test_trainer_evaluate_metrics, f)\n",
    "\n",
    "    print(f\"{name} results successfully saved locally to {test_trainer_evaluate_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = test_trainer_evaluate_path,\n",
    "        path_in_repo    = test_trainer_evaluate_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} results successfully uploaded to HF Hub as {test_trainer_evaluate_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJNMb3idDapI"
   },
   "source": [
    "### Test 2: trainer.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s9jKJjJACguS",
    "outputId": "badbe1c2-8322-48ca-ffbe-5a906311f3f2"
   },
   "outputs": [],
   "source": [
    "optimized_thresholds, optimized_thresholds_dict, test_trainer_predict_metrics, test_trainer_predict_global_metrics = predict_with_optimized_thresholds(\n",
    "    trainer, test_dataset, id2label, threshold_tuning=False, thresholds=optimized_thresholds)\n",
    "\n",
    "print(f\"optimized_thresholds: {type(optimized_thresholds)} shape={optimized_thresholds.shape} {optimized_thresholds}\")\n",
    "print(f\"optimized_thresholds_dict: {type(optimized_thresholds_dict)} len={len(optimized_thresholds_dict)}\\n{optimized_thresholds_dict}\")\n",
    "print(f\"test_trainer_predict_metrics: {type(test_trainer_predict_metrics)} len={len(test_trainer_predict_metrics)}\\n{json.dumps(test_trainer_predict_metrics, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9lfg-tYjaWC"
   },
   "outputs": [],
   "source": [
    "#test_trainer_predict_results = predict_with_optimized_thresholds(\n",
    "#    trainer, test_dataset, id2label, threshold_tuning=False, thresholds=optimized_thresholds)\n",
    "\n",
    "#except_report = {k: v for k, v in test_trainer_predict_results.items() if k!='classification_report'}\n",
    "#report        = test_trainer_predict_results['classification_report']\n",
    "#print(f\"test_trainer_predict_results: {type(except_report)} len={len(except_report)}\\n{json.dumps(except_report, indent=4)}\")\n",
    "#print(f\"test_trainer_predict_results['classification_report']: {type(report)} len={len(report)}\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fs3FWd4mfUu",
    "outputId": "70b2e897-eff2-4bcd-83f1-d1cfc3d259c3"
   },
   "outputs": [],
   "source": [
    "print(\"test_trainer.predict successfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr9L2hoGoQF-"
   },
   "source": [
    "### Test 2: trainer.predict: save locally and upload to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq1WgCmgoioR",
    "outputId": "1d93a65a-dd72-44e9-bfb2-2a0e9678afc0"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "    name                      = \"test_trainer_predict\"\n",
    "    test_trainer_predict_path = f\"{name}.json\"\n",
    "\n",
    "    with open(test_trainer_predict_path, \"w\") as f:\n",
    "        json.dump(test_trainer_predict_metrics, f)\n",
    "\n",
    "    print(f\"{name} results successfully saved locally to {test_trainer_predict_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = test_trainer_predict_path,\n",
    "        path_in_repo    = test_trainer_predict_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} results successfully uploaded to HF Hub as {test_trainer_predict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAvjnQsiPvA3"
   },
   "source": [
    "### Test 3: model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEmK1IiGPBQ5",
    "outputId": "290426d4-a0a2-4d11-e2e9-fa6de2076c02"
   },
   "outputs": [],
   "source": [
    "test_model_eval_metrics = compute_metrics_with_threshold(model, test_dataset, optimized_thresholds, id2label, batch_size=16)\n",
    "\n",
    "except_report = {k: v for k, v in test_model_eval_metrics.items() if k!='classification_report'}\n",
    "report        = test_model_eval_metrics['classification_report']\n",
    "print(f\"test_model_eval_metrics: {type(except_report)} len={len(except_report)}\\n{json.dumps(except_report, indent=4)}\")\n",
    "print(f\"test_model_eval_metrics['classification_report']: {type(report)} len={len(report)}\\n{report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xsR393boVNL"
   },
   "source": [
    "### Test 3: model.eval: save locally and upload to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjInlLbeoSw3",
    "outputId": "2e59f4cc-86ce-4450-f9db-88f2a0cd8eff"
   },
   "outputs": [],
   "source": [
    "if upload_to_HF is True:\n",
    "\n",
    "    name                 = \"test_model_eval\"\n",
    "    test_model_eval_path = f\"{name}.json\"\n",
    "\n",
    "    with open(test_model_eval_path, \"w\") as f:\n",
    "        json.dump(test_model_eval_metrics, f)\n",
    "\n",
    "    print(f\"{name} successfully saved locally to {test_model_eval_path}\")\n",
    "\n",
    "    upload_file(\n",
    "        path_or_fileobj = test_model_eval_path,\n",
    "        path_in_repo    = test_model_eval_path,\n",
    "        repo_id         = repo_id,\n",
    "        repo_type       = 'dataset',\n",
    "        commit_message  = f\"{name}_{timestamp}\"\n",
    "    )\n",
    "\n",
    "    print(f\"{name} successfully uploaded to HF Hub as {test_model_eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "vvh_jDvdBt26",
    "outputId": "637f825c-3561-4d56-dfc2-a4183145bbe9"
   },
   "outputs": [],
   "source": [
    "raise Exception(\"It's the end, I stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9CetqX9zAM3"
   },
   "source": [
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqxhHkT1wcJ4"
   },
   "outputs": [],
   "source": [
    "# Define the weighted loss function\n",
    "\n",
    "class_weights = torch.tensor([7.68, 2.15, 0.61, 0.47, 0.68, 6.26], dtype=torch.float32).to(device)\n",
    "loss_fn       = BCEWithLogitsLoss(pos_weight=class_weights)  # For multi-label classification (binary classification per label)\n",
    "\n",
    "## Class supports, class weigths, weighted loss function\n",
    "\n",
    "#Reminder:\n",
    "#*   df_jobs      : <class 'pandas.core.frame.DataFrame'>\n",
    "#*   df_jobs['id']: <class 'pandas.core.series.Series'>\n",
    "\n",
    "#dataset = Dataset.from_pandas(df_jobs)\n",
    "#*   dataset      : <class 'datasets.arrow_dataset.Dataset'>\n",
    "#*   dataset['id']: <class 'list'>\n",
    "\n",
    "#*   dataset_dict_jobs : <class 'datasets.dataset_dict.DatasetDict'>\n",
    "#*   train_dataset     : <class 'datasets.arrow_dataset.Dataset'>\n",
    "#*   validation_dataset: <class 'datasets.arrow_dataset.Dataset'>\n",
    "#*   test_dataset      : <class 'datasets.arrow_dataset.Dataset'>\n",
    "\n",
    "\n",
    "#We calculate the class supports for the train, validation and test datasets; the class weights and the weighted loss function are used for training only; the class supports of validation_dataset and test_dataset are calculated for information only.\n",
    "# function B\n",
    "def get_train_class_weights(datasetDict, labels):\n",
    "  print(f\"datasetDict: {type(datasetDict)} shape={datasetDict.shape}\\n{datasetDict}\")\n",
    "  print(f\"labels: {type(labels)} len={len(labels)}\\n{labels}\")\n",
    "\n",
    "  dataset_train      = datasetDict['train']\n",
    "  dataset_validation = datasetDict['validation']\n",
    "  dataset_test       = datasetDict['test']\n",
    "\n",
    "  def calculate_class_supports(dataset, labels):\n",
    "    class_supports = dataset.map(\n",
    "        lambda example: {col: example[col] for col in labels},\n",
    "        batched=True\n",
    "    ).to_pandas()[labels].sum(axis=0)\n",
    "    return class_supports\n",
    "\n",
    "  class_supports = {}\n",
    "\n",
    "  for split_name, split_dataset in datasetDict.items():\n",
    "    class_supports[split_name] = calculate_class_supports(split_dataset, labels)\n",
    "\n",
    "  for split_name, split_class_supports in class_supports.items():\n",
    "    print(f\"{split_name}: {type(split_class_supports)} len={len(split_class_supports)}\\n{split_class_supports}\")\n",
    "\n",
    "  train_class_supports_list = class_supports['train'].tolist()\n",
    "  print(f\"train_class_supports_list: {type(train_class_supports_list)} len={len(train_class_supports_list)} {train_class_supports_list}\")\n",
    "\n",
    "  train_class_supports_tensor = torch.tensor(train_class_supports_list, dtype=torch.float32)\n",
    "  print(f\"train_class_supports_tensor: {type(train_class_supports_tensor)} len={len(train_class_supports_tensor)} {train_class_supports_tensor}\")\n",
    "\n",
    "  train_total_samples = dataset_train.num_rows\n",
    "  print(f\"train_total_samples: {train_total_samples}\")\n",
    "\n",
    "  number_of_classes = len(labels)\n",
    "  print(f\"number_of_classes: {number_of_classes}\")\n",
    "\n",
    "  train_class_weights = train_total_samples / (number_of_classes * train_class_supports_tensor)\n",
    "  print(f\"train_class_weights: {type(train_class_weights)} len={len(train_class_weights)} {train_class_weights}\")\n",
    "\n",
    "  train_class_weights_sum = train_class_weights.sum()\n",
    "  print(f\"train_class_weights_sum: {train_class_weights_sum}\")\n",
    "\n",
    "  normalized_train_class_weights = (train_class_weights / train_class_weights_sum) * number_of_classes\n",
    "  print(f\"normalized_train_class_weights: {type(normalized_train_class_weights)} len={len(normalized_train_class_weights)} {normalized_train_class_weights}\")\n",
    "\n",
    "  # Positives samples per label\n",
    "  supports = train_class_supports_tensor\n",
    "  print(f\"supports: {type(supports)} {len(supports)} {supports}\")\n",
    "\n",
    "  # Negatives samples per label\n",
    "  negatives = train_total_samples - supports\n",
    "  print(f\"negatives: {type(negatives)} {len(negatives)} {negatives}\")\n",
    "\n",
    "  # pos_weights = negative to positive ratios\n",
    "  pos_weights = negatives/supports\n",
    "  print(f\"pos_weights: {type(pos_weights)} {len(pos_weights)} {pos_weights}\")\n",
    "\n",
    "  # Normalize using min-max scaling\n",
    "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
    "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
    "\n",
    "  # Normalize using z-score standardization\n",
    "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
    "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
    "\n",
    "  # Normalize using min-max scaling\n",
    "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
    "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
    "\n",
    "  # Normalize using z-score standardization\n",
    "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
    "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
    "\n",
    "  # Normalize using sum-to-one\n",
    "  normalized_pos_weights_sum1 = pos_weights / pos_weights.sum()\n",
    "  print(f\"normalized_pos_weights_sum1: {type(normalized_pos_weights_sum1)} {len(normalized_pos_weights_sum1)} {normalized_pos_weights_sum1}\")\n",
    "\n",
    "  return normalized_pos_weights_minmax\n",
    "  #return normalized_pos_weights_zscore\n",
    "  #return normalized_pos_weights_sum1\n",
    "\n",
    "pos_weights = get_train_class_weights(datasetDict, labels)\n",
    "\n",
    "loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights.to(device))  # For multi-label classification (binary classification per label)\n",
    "print(f\"loss_fn: {type(loss_fn)} {loss_fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5BOwuD1RnQe"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data from your training results\n",
    "epochs = np.arange(1, 9)\n",
    "training_loss = [0.3122, 0.2897, 0.2665, 0.2466, 0.2244, 0.2223, 0.2202, 0.2077]\n",
    "validation_loss = [0.303785, 0.293599, 0.278830, 0.275663, 0.280968, 0.280640, 0.279608, 0.282026]\n",
    "f1_score = [0.865113, 0.871222, 0.875554, 0.880279, 0.879128, 0.878554, 0.879872, 0.877893]\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, f1_score, label='F1 Score', marker='o', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVn29oBT2n0k"
   },
   "outputs": [],
   "source": [
    "example = datasetDict['test'][0]\n",
    "print(f\"datasetDict['test'][0]: {type(example)} {example.keys()}\\n{example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spiVOeMhz6Ku"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    example['text'],\n",
    "    truncation     = True,\n",
    "    padding        = 'max_length',\n",
    "    max_length     = max_length,\n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2U-0iYO0LWf"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient calculation during prediction\n",
    "    outputs = model(\n",
    "        input_ids=inputs.input_ids.to(device),\n",
    "        attention_mask=inputs.attention_mask.to(device)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-VuKUM10dte"
   },
   "outputs": [],
   "source": [
    "probs = torch.sigmoid(outputs.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMOj7NB11CH8"
   },
   "outputs": [],
   "source": [
    "best_thresholds = [0.4, 0.3, 0.3, 0.3, 0.4, 0.5]\n",
    "preds = np.zeros_like(probs)  # Initialize predictions array\n",
    "for label_idx in range(num_labels):\n",
    "  preds[:, label_idx] = (probs[:, label_idx] > best_thresholds[label_idx])  #.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICCj8Hm3i_GP"
   },
   "outputs": [],
   "source": [
    "print(f\"probs: {type(probs)} shape={probs.shape}\\n{probs}\")\n",
    "print(f\"preds: {type(preds)} shape={preds.shape}\\n{preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiCmBf-kjcfc"
   },
   "outputs": [],
   "source": [
    "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")\n",
    "# '390': False, '135': False, '136': True, '137': True, '138': True, '139': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJFDabZ41cbP"
   },
   "outputs": [],
   "source": [
    "def tune_thresholds(true_labels, probs, id2label):\n",
    "  \"\"\"\n",
    "  Tune thresholds for each label to maximize F1 alone, as F1 balances precision and recall into a single metric.\n",
    "\n",
    "  Args:\n",
    "    true_labels: actual labels for the data                                      (numpy array of shape (num_samples, num_labels))\n",
    "    probs      : predicted probabilities                                         (numpy array of shape (num_samples, num_labels))\n",
    "    id2label   : dictionary mapping label indices (int) to label names (string)\n",
    "\n",
    "  Returns:\n",
    "    best_thresholds: best threshold for each label                                                      (numpy array of shape (num_labels,))\n",
    "    best_metrics   : dictionary of best F1, precision_for_best_f1 and recall_for_best_f1 for each label (dictionary of numpy arrays)\n",
    "  \"\"\"\n",
    "  thresholds      = np.linspace(0.1, 0.9, 9)\n",
    "  best_thresholds = np.zeros(len(id2label))\n",
    "  best_metrics    = {label: {'f1': 0.0, 'precision': 0.0, 'recall': 0.0} for label in id2label.values()}\n",
    "\n",
    "  for label_idx, label in id2label.items():\n",
    "    for threshold in thresholds:\n",
    "      pred                     = (probs[:, label_idx] > threshold).astype(int)\n",
    "      precision, recall, f1, _ = precision_recall_fscore_support(true_labels[:, label_idx], pred, average='binary', zero_division=0)\n",
    "      if f1 > best_metrics[label]['f1']:\n",
    "        best_thresholds[label_idx]       = threshold\n",
    "        best_metrics[label]['f1']        = f1\n",
    "        best_metrics[label]['precision'] = precision\n",
    "        best_metrics[label]['recall']    = recall\n",
    "\n",
    "  print(\"==== tune_thresholds ====\")\n",
    "  print(f\"best_thresholds: {type(best_thresholds)} shape={best_thresholds.shape}\\n{best_thresholds}\")\n",
    "  print(f\"best_metrics   : {type(best_metrics)}    len={len(best_metrics)}      \\n{json.dumps(best_metrics, indent=4)}\")\n",
    "  print(\"=========================\")\n",
    "  print()\n",
    "\n",
    "  return best_thresholds, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBsBg2vL1y6s"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_with_threshold_OLD(probs, label_ids, thresholds, id2label):\n",
    "    \"\"\"\n",
    "    Compute metrics during evaluation or test, by applying tuned thresholds\n",
    "\n",
    "    average:\n",
    "    - 'micro'   : gives more weight to frequent labels     → best for imbalanced datasets where frequent labels are more important\n",
    "    - 'macro'   : treats all labels equally                → best when you care about rare labels as much as frequent ones\n",
    "    - 'weighted': like macro but considers label frequency → best if you want a compromise between macro and micro\n",
    "\n",
    "    - 'macro' or 'weighted' AUC is often best because AUC isn't as affected by class imbalance as F1/Precision/Recall\n",
    "    - 'macro'      AUC: usually the best because it treats all labels equally, avoiding the dominance of frequent labels\n",
    "    - 'weighted'   AUC: similar to macro but considers label frequency\n",
    "    - 'macro'   PR AUC: best for imbalanced datasets because it treats rare labels fairly\n",
    "    - 'weighted PR AUC: also good, but slightly biased toward frequent labels\n",
    "\n",
    "    PR AUC is better than ROC AUC when you care about positive examples in imbalanced data\n",
    "    \"\"\"\n",
    "    average = 'macro'\n",
    "    preds   = np.zeros_like(probs)\n",
    "\n",
    "    # Apply per-label tuned threshold\n",
    "    for label_idx in id2label.keys():\n",
    "        preds[:, label_idx] = (probs[:, label_idx] > thresholds[label_idx]).astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    f1        = f1_score       (label_ids, preds, average=average)\n",
    "    precision = precision_score(label_ids, preds, average=average)\n",
    "    recall    = recall_score   (label_ids, preds, average=average)\n",
    "    accuracy  = accuracy_score (label_ids, preds)\n",
    "\n",
    "    # Compute AUC scores with error handling\n",
    "    try:\n",
    "        roc_auc              = roc_auc_score          (label_ids, probs, average=average)\n",
    "    except ValueError:\n",
    "        roc_auc              = 0.0\n",
    "\n",
    "    try:\n",
    "        precision_recall_auc = average_precision_score(label_ids, probs, average=average)\n",
    "    except ValueError:\n",
    "        precision_recall_auc = 0.0\n",
    "\n",
    "    # Compute per-class metrics (average = None)\n",
    "    per_class_f1        = f1_score       (label_ids, preds, average=None)\n",
    "    per_class_precision = precision_score(label_ids, preds, average=None)\n",
    "    per_class_recall    = recall_score   (label_ids, preds, average=None)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(label_ids, preds, target_names=id2label.values(), zero_division=0)\n",
    "\n",
    "    # Store metrics\n",
    "    metrics = {\n",
    "        'f1'                   : f1,\n",
    "        'precision'            : precision,\n",
    "        'recall'               : recall,\n",
    "        'accuracy'             : accuracy,\n",
    "        'roc_auc'              : roc_auc,\n",
    "        'precision_recall_auc' : precision_recall_auc,\n",
    "        'thresholds'           : thresholds.tolist(),\n",
    "        'classification_report': report,\n",
    "        'per_class_f1'         : per_class_f1,\n",
    "        'per_class_precision'  : per_class_precision,\n",
    "        'per_class_recall'     : per_class_recall\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OYJ-0tP2E7c"
   },
   "outputs": [],
   "source": [
    "def predict_with_optimized_thresholds_cuda(trainer, dataset, threshold_tuning=False, thresholds=None, threshold=threshold):\n",
    "    \"\"\"\n",
    "    Predicts using trainer.predict(), with optional threshold tuning on GPU\n",
    "\n",
    "    Parameters:\n",
    "    - trainer         : Hugging Face Trainer or CustomTrainer instance\n",
    "    - dataset         : dataset to predict on\n",
    "    - id2label        : dictionary mapping label indices (int) to label names (string)\n",
    "    - threshold_tuning: boolean to enable thresholds tuning per class (if evaluation, True, if prediction, False)\n",
    "    - thresholds      : if evaluation, custom thresholds, if prediction, tuned thresholds (from evaluation)\n",
    "\n",
    "    Returns:\n",
    "    - best_thresholds (if threshold_tuning=True): optimized threshold for each label\n",
    "    - best_metrics                              : computed with tuned thresholds whether for evaluation or prediction\n",
    "\n",
    "    - metrics (if threshold_tuning=False): computed with fixed thresholds\n",
    "    - metrics (if threshold_tuning=True): computed with tuned thresholds\n",
    "    - predictions: final binary predictions\n",
    "    - label_ids  : ground true labels from the dataset\n",
    "    - best_thresholds (if threshold_tuning=True): optimized threshold per class\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Auto-detect GPU\n",
    "\n",
    "    # Predict\n",
    "    predictions_output = trainer.predict(dataset)        # <class 'transformers.trainer_utils.PredictionOutput'> len   = 3\n",
    "    predictions_np     = predictions_output.predictions  # <class 'numpy.ndarray'>                               shape = (2400, 6)\n",
    "    label_ids_np       = predictions_output.label_ids    # <class 'numpy.ndarray'>                               shape = (2400, 6)\n",
    "    metrics_dict       = predictions_output.metrics      # <class 'dict'> (= trainer.evaluate results)           len   = 10\n",
    "\n",
    "    print(\"==== predictions_output ====\")\n",
    "    print(f\"predictions_output.predictions: {type(predictions_np)} shape={predictions_np.shape} \\n{predictions_np}\")\n",
    "    print(f\"predictions_output.label_ids  : {type(label_ids_np)}   shape={label_ids_np.shape}   \\n{label_ids_np}\")\n",
    "    print(f\"predictions_output.metrics    : {type(metrics_dict)}   len={len(metrics_dict)}      \\n{json.dumps(metrics_dict, indent=4)}\")\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors (torch.from_numpy() keeps the NumPy array's memory layout, while torch.tensor() creates a new copy)\n",
    "    logits      = torch.tensor(predictions_np, device=device)  # Move to GPU\n",
    "    true_labels = torch.tensor(label_ids_np, device=device)    # Move to GPU\n",
    "\n",
    "    print(f\"logits     : {type(logits)}      shape={logits.shape}     \\n{logits}\")\n",
    "    print(f\"true_labels: {type(true_labels)} shape={true_labels.shape}\\n{true_labels}\")\n",
    "\n",
    "    # Convert logits to probabilities using PyTorch (on GPU)\n",
    "    probs = torch.sigmoid(logits)  # <class 'torch.Tensor'>  shape = (1200, 6)\n",
    "\n",
    "    print(f\"probs: {type(probs)} shape={probs.shape}\\n{probs}\")\n",
    "    print(\"============================\")\n",
    "    print()\n",
    "\n",
    "    num_labels = probs.shape[1]\n",
    "\n",
    "    if threshold_tuning:\n",
    "        best_thresholds = torch.full((num_labels,), threshold, device=device, dtype=torch.float32)  # Default to threshold\n",
    "\n",
    "        # Define candidate thresholds (on GPU)\n",
    "        threshold_candidates = torch.linspace(0.05, 0.95, 19, device=device)\n",
    "\n",
    "        for label_idx in range(num_labels):\n",
    "            best_f1 = 0\n",
    "            for threshold in threshold_candidates:\n",
    "                preds = (probs[:, label_idx] > threshold).int()\n",
    "\n",
    "                # Convert to CPU for sklearn\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                    true_labels[:, label_idx].cpu().numpy(),\n",
    "                    preds.cpu().numpy(),\n",
    "                    average='binary',\n",
    "                    zero_division=0\n",
    "                )\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1                    = f1\n",
    "                    best_thresholds[label_idx] = threshold  # Store best threshold\n",
    "\n",
    "        return probs, true_labels, best_thresholds\n",
    "\n",
    "    # Apply provided thresholds (or default to 0.5)\n",
    "    if thresholds is None:\n",
    "        thresholds = torch.full((num_labels,), 0.5, device=device)  # Default to 0.5\n",
    "    else:\n",
    "        thresholds = torch.tensor(thresholds, device=device)  # Move thresholds to GPU\n",
    "\n",
    "    preds = (probs > thresholds).int()\n",
    "\n",
    "    return preds, true_labels, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwTLn73u1n6m"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_with_threshold_SAV(probs, label_ids, thresholds, id2label):\n",
    "\n",
    "  #Compute metrics during evaluation or test, by applying tuned thresholds\n",
    "\n",
    "  #logits  = eval_preds.predictions\n",
    "  #labels  = eval_preds.label_ids\n",
    "  #sigmoid = torch.nn.Sigmoid  # Sigmoid or numpy?\n",
    "  #probs   = sigmoid(logits).cpu().numpy()\n",
    "  preds   = np.zeros_like(probs)\n",
    "\n",
    "  if threshold_tuning:\n",
    "    # Apply per-label tuned threshold\n",
    "    for label_idx in id2label.keys():\n",
    "        preds[:, label_idx] = (probs[:, label_idx] > thresholds[label_idx]).astype(int)\n",
    "  else:\n",
    "    # threhsolds = None, apply a fixed threshold to all labels\n",
    "    for label_idx in id2label.keys():\n",
    "        preds[:, label_idx] = (probs[:, label_idx] > threshold).astype(int)\n",
    "\n",
    "  # Compute metrics\n",
    "  f1                    = f1_score               (label_ids, preds, average='micro')\n",
    "  precision             = precision_score        (label_ids, preds, average='micro')\n",
    "  recall                = recall_score           (label_ids, preds, average='micro')\n",
    "  accuracy              = accuracy_score         (label_ids, preds)\n",
    "  roc_auc               = roc_auc_score          (label_ids, probs, average='micro')  # multi_class=\"ovr\" <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "  precision_recall_auc  = average_precision_score(label_ids, probs, average='micro')\n",
    "\n",
    "  # Use id2label for target_names\n",
    "  report = classification_report(label_ids, preds, target_names=id2label.values(), zero_division=0)\n",
    "\n",
    "  if threshold_tuning:\n",
    "    _thresholds = thresholds.tolist()\n",
    "  else:\n",
    "    _thresholds = threshold\n",
    "\n",
    "  metrics = {\n",
    "      'f1'                   : f1,\n",
    "      'precision'            : precision,\n",
    "      'recall'               : recall,\n",
    "      'accuracy'             : accuracy,\n",
    "      'roc_auc'              : roc_auc,\n",
    "      'precision_recall_auc' : precision_recall_auc,\n",
    "      'thresholds'           : _thresholds,\n",
    "      'classification_report': report\n",
    "  }\n",
    "\n",
    "  return metrics"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:nomarker"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
